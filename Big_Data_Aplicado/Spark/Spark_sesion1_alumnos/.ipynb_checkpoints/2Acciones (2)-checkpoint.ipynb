{"cells":[{"cell_type":"code","execution_count":null,"id":"301bddf4-4487-4ffe-b19d-50f8d4782e28","metadata":{"id":"301bddf4-4487-4ffe-b19d-50f8d4782e28"},"outputs":[],"source":["\n","\n","\n","\n"]},{"cell_type":"markdown","id":"iNyzo2zCh3TY","metadata":{"id":"iNyzo2zCh3TY"},"source":["Acciones\n","\n","https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions\n","\n","\n","reduce(func) -\tAggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.\n","\n","collect()\tReturn all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.\n","\n","count()\tReturn the number of elements in the dataset.\n","\n","first()\tReturn the first element of the dataset (similar to take(1)).\n","\n","take(n)\tReturn an array with the first n elements of the dataset.\n","\n","takeSample(withReplacement, num, [seed])\tReturn an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.\n","\n","takeOrdered(n, [ordering])\tReturn the first n elements of the RDD using either their natural order or a custom comparator.\n","\n","saveAsTextFile(path)\tWrite the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.\n","\n","saveAsSequenceFile(path)\n","(Java and Scala)\tWrite the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).\n","\n","saveAsObjectFile(path)\n","(Java and Scala)\tWrite the elements of the dataset in a simple format using Java serialization, which can then be loaded using SparkContext.objectFile().\n","\n","countByKey()\tOnly available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.\n","\n","foreach(func)\tRun a function func on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems.\n","\n","Note: modifying variables other than Accumulators outside of the foreach() may result in undefined behavior. See Understanding closures for more details.\n"]},{"cell_type":"code","execution_count":1,"id":"afbf96cb-277f-4ea9-9e30-78c8bc910425","metadata":{"id":"afbf96cb-277f-4ea9-9e30-78c8bc910425","outputId":"552c2975-c409-4b74-9ed3-d9ebec77a602"},"outputs":[{"name":"stderr","output_type":"stream","text":["24/01/12 18:27:04 WARN Utils: Your hostname, A208-100 resolves to a loopback address: 127.0.1.1; using 192.168.208.123 instead (on interface eno1)\n","24/01/12 18:27:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/01/12 18:27:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","24/01/12 18:27:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n","24/01/12 18:27:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n","24/01/12 18:27:06 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"]},{"name":"stdout","output_type":"stream","text":["<pyspark.sql.session.SparkSession object at 0x7fca9851b220>\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 0:=========>                                                 (1 + 5) / 6]\r"]},{"name":"stdout","output_type":"stream","text":["6\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["[1, 2, 3, 4, 5, 6]"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["#inicializamos una aplicacion spark\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"Spark1\").getOrCreate()\n","print(spark)\n","\n","\n","# creamos un RDD a traves de la funcion parallelize\n","# que recibe como parámetro un array\n","\n","rddNum = spark.sparkContext.parallelize([1,2,3,4,5,6])\n","\n","\n","myarray=[\"haddoop\",\"bigdata\",\"kafka\",\"spark\",\"data\", \"haddoop\",\"kibana\", \"RDD\",\"Mongo\",\"Tableau\",\"nodered\",\"kafka\", \"spark\",\"elastic\",\"bigdata\",\"IESAbastos\",\"IESAbastos\",\"IESAbastos\"]\n","\n","\n","rddString = spark.sparkContext.parallelize(myarray)\n","\n","## con count sacamos el número de elementos\n","print (rddNum.count())\n","\n","# con collect()  obtenemos el contenod del array\n","rddNum.collect()\n"]},{"cell_type":"markdown","id":"BKL4pLtxh2Ui","metadata":{"id":"BKL4pLtxh2Ui"},"source":[]},{"cell_type":"code","execution_count":null,"id":"5fc062a3-7eb4-4379-88ce-0c0c6964dd19","metadata":{"id":"5fc062a3-7eb4-4379-88ce-0c0c6964dd19","outputId":"becbe258-981d-433f-f027-20beba2450a0"},"outputs":[],"source":["#Acion collect()  obtenemos el contenod del array\n","rddNum.collect()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"a3b4f634-3927-4b1c-9b18-63ae08db1849","metadata":{"id":"a3b4f634-3927-4b1c-9b18-63ae08db1849","outputId":"5c15f749-4d6b-4ab3-b220-63025e8a938e"},"outputs":[],"source":["#action first()   --> nos devuelve el primer elemento\n","rddNum.first()\n"]},{"cell_type":"code","execution_count":null,"id":"659330ab-8b60-4cbe-bffc-589611439b6e","metadata":{"id":"659330ab-8b60-4cbe-bffc-589611439b6e","outputId":"d2561670-d01b-4f4b-a03f-44e3564248f1"},"outputs":[],"source":["#accion count ()  --> nos devuelve el numero de elemento\n","rddNum.count()\n"]},{"cell_type":"code","execution_count":null,"id":"bbcb3068-4459-40c3-aeb2-6d596172dab6","metadata":{"id":"bbcb3068-4459-40c3-aeb2-6d596172dab6","outputId":"0f818271-ecb5-4845-ef46-7aea517af776"},"outputs":[],"source":["#accion Reduce --> aplica la misma operacion a todos los elementos del rdd\n","\n","rddNum.reduce(lambda x,y : x + y)\n"]},{"cell_type":"code","execution_count":null,"id":"f184a05e-5616-4d73-8e05-edf117991f46","metadata":{"id":"f184a05e-5616-4d73-8e05-edf117991f46","outputId":"7075a24e-69a4-4b60-b360-eb1d32ebfa17"},"outputs":[],"source":["#accion take  -- recupera n elementos\n","rddNum.take(3)"]},{"cell_type":"code","execution_count":null,"id":"bf466acb-8cab-4a22-b3c5-cde32b2e887d","metadata":{"id":"bf466acb-8cab-4a22-b3c5-cde32b2e887d","outputId":"9ad974ab-9efe-47be-c65b-3efd2a32e8b9"},"outputs":[],"source":["#accion max   --> devuelve el máximo\n","rddNum.max()\n"]},{"cell_type":"code","execution_count":null,"id":"06112c7f-326f-48dd-bdc0-6f09d9555f2c","metadata":{"id":"06112c7f-326f-48dd-bdc0-6f09d9555f2c","outputId":"6525fafa-f315-4c8e-d9b5-792a24b2da98"},"outputs":[],"source":["#accion min   --> devuelve el minimo\n","rddNum.min()"]},{"cell_type":"code","execution_count":null,"id":"f69083bc-59a4-4153-8127-7f1f67fe9ffd","metadata":{"id":"f69083bc-59a4-4153-8127-7f1f67fe9ffd","outputId":"594c5276-fe5d-4f47-a6c4-92920bac7399"},"outputs":[],"source":["#foreach recorre todos los elementos\n","\n","rddString.foreach (lambda x: print(\" hola \" +x) )\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"91a3a706-44fb-4927-abfa-f95a1afd96e9","metadata":{"id":"91a3a706-44fb-4927-abfa-f95a1afd96e9","outputId":"f54e85ad-246a-45b5-c777-1f7fb18bb800"},"outputs":[],"source":["#selecciona una muestra de elementos\n","\n","rddSample = rddString.takeSample(False,5)\n","rddSample"]},{"cell_type":"code","execution_count":null,"id":"2965758a-4827-4e7c-b5f2-a883a812b2d6","metadata":{"id":"2965758a-4827-4e7c-b5f2-a883a812b2d6"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
