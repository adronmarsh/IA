{"cells":[{"cell_type":"markdown","id":"8363fc3c-431b-45e6-8f6d-a949a0a02d4a","metadata":{"id":"8363fc3c-431b-45e6-8f6d-a949a0a02d4a"},"source":["# Libreta con ejercicios de transformacions en sparl\n","\n"]},{"cell_type":"markdown","id":"356b9931-795b-4ed0-811d-71cf870884ab","metadata":{"id":"356b9931-795b-4ed0-811d-71cf870884ab"},"source":["### Referencia a la api de spark\n","\n","Lo primero que tenemos que hacer siempre en una aplicacion spark es crear una sesion de spark y ponerle nombre a la aplicacion,\n","\n","\n","En spark existen 3 maneras de crear un RDD:\n","\n","<B>1)</B> mediante la funcion parallelize sobre una lista\n","\n","myarray = [\"a\",\"b\",\"c\",\"d\",\"e\"]\n","spark.sparkContext.parallelize(myarray)\n","\n","\n","<B>2)</B> Referenciando a un fichero que existe\n","\n","RDD = sc.textFile(\"  / /\")\n","\n","\n","<B>3)</B> Creando un RDD desde otro RDD existente\n","RDD = RDD1.map(lambda x: x +1)\n","\n","\n","\n","\n","https://spark.apache.org/docs/latest/api/python/reference/pyspark.html\n","\n","\n"]},{"cell_type":"code","execution_count":1,"id":"d2caf8bd-12f1-4ebd-90a0-ac3f0171ed25","metadata":{"id":"d2caf8bd-12f1-4ebd-90a0-ac3f0171ed25","outputId":"2580b1c4-6421-4149-c8c7-5979b48b7379"},"outputs":[{"name":"stdout","output_type":"stream","text":["<pyspark.sql.session.SparkSession object at 0x7f536fd7f3d0>\n","6\n"]},{"data":{"text/plain":["[1, 2, 3, 4, 5, 6]"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["#inicializamos una aplicacion spark\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"Spark1\").getOrCreate()\n","print(spark)\n","\n","\n","# creamos un RDD a traves de la funcion parallelize\n","# que recibe como parámetro un array\n","\n","rdd = spark.sparkContext.parallelize([1,2,3,4,5,6])\n","\n","## con count sacamos el número de elementos\n","print (rdd.count())\n","\n","# con collect()  obtenemos el contenod del array\n","rdd.collect()\n"]},{"cell_type":"markdown","id":"51c35188-ad67-4f2c-a881-bcb0adcbfc4d","metadata":{"id":"51c35188-ad67-4f2c-a881-bcb0adcbfc4d"},"source":["## 1.1. Creamos una cadena de palabras en python que usaremos para las diferentes transformaciones\n","\n","a) crear un array que contenga entre 10-20 palabras, algunas deben de estar repetidas varias veces.\n","\n","myarray=[\"haddoop\",\"bigdata\",\"kafka\",\"spark\",\"data\", \"haddoop\",\"kibana\", \"RDD\",\"Mongo\",\"Tableau\",\"nodered\",\"kafka\", \"spark\",\"elastic\",\"bigdata\",\"IESAbastos\",\"IESAbastos\",\"IESAbastos\"]\n","\n","b) Una vez tengamos el array hay que crear un RDD por medio de la funcion parallelize.\n","\n","c) sacamos por pantalla el número de elementos del array\n","\n","\n"]},{"cell_type":"code","execution_count":2,"id":"48ad753f-dd39-4bc5-bb6a-d75c9803173b","metadata":{"id":"48ad753f-dd39-4bc5-bb6a-d75c9803173b"},"outputs":[{"name":"stdout","output_type":"stream","text":["18\n"]}],"source":["myarray=[\"haddoop\",\"bigdata\",\"kafka\",\"spark\",\"data\", \"haddoop\",\"kibana\", \"RDD\",\"Mongo\",\"Tableau\",\"nodered\",\"kafka\", \"spark\",\"elastic\",\"bigdata\",\"IESAbastos\",\"IESAbastos\",\"IESAbastos\"]\n","\n","myRDD = spark.sparkContext.parallelize(myarray)\n","\n","print(myRDD.count())"]},{"cell_type":"markdown","id":"59438448-37d6-4f08-b56f-ce73e39890a5","metadata":{"id":"59438448-37d6-4f08-b56f-ce73e39890a5"},"source":["## 1.2 Tansformaciones MAP\n","\n","Gracias a la funcion map podemos transformar de manera simple todos los elementos de un rdd.\n","Para las transformaciones en pyspark se suele usar una funcion lambda:\n","             (lamba   x: \"transformacion a aplicar\")\n","\n","REF- lambda: https://www.freecodecamp.org/espanol/news/expresiones-lambda-en-python/\n","- https://docs.python.org/es/3/reference/expressions.html#lambda\n","\n","\n","\n","\n","<b>ejercicio 1</b> Transformar todos los elementos a mayuscula\n","\n","\n"]},{"cell_type":"code","execution_count":3,"id":"17acc0d4-6a7f-4ba0-8dee-cfea155b7b7a","metadata":{"id":"17acc0d4-6a7f-4ba0-8dee-cfea155b7b7a"},"outputs":[],"source":["myRDDMayusculas = myRDD.map(lambda x: x.upper())"]},{"cell_type":"markdown","id":"0b763e39-5510-406e-8540-3794ce5c8659","metadata":{"id":"0b763e39-5510-406e-8540-3794ce5c8659"},"source":["## 1.3  Transformacion filter\n","    \n","mediante la transformacion filter podemos filtrar los elemento sde la cadana en base a una condició (\"que empieze por\", \"que contenga\", que acabe)\n","    \n","\n","\n","Ejercicio2:\n","- realizar un RDD que contenga todos los elementos que empiecen por un string \"B\".\n","-\n","\n","- realizar un RDD que contenga todos los elementos que terminen por string \"A\"\n","\n","\n","\n","    "]},{"cell_type":"code","execution_count":4,"id":"f2b7b710-707e-4a21-90b6-56cc8b958a00","metadata":{"id":"f2b7b710-707e-4a21-90b6-56cc8b958a00"},"outputs":[{"name":"stdout","output_type":"stream","text":["Empiezan con \"B\": ['BIGDATA', 'BIGDATA']\n","Terminan con \"A\": ['BIGDATA', 'KAFKA', 'DATA', 'KIBANA', 'KAFKA', 'BIGDATA']\n"]}],"source":["#Ejercicio 2\n","ConB = myRDDMayusculas.filter(lambda x: x.startswith(\"B\"))\n","print('Empiezan con \"B\":', ConB.collect())\n","finA = myRDDMayusculas.filter(lambda x:x.endswith(\"A\"))\n","print('Terminan con \"A\":', finA.collect())"]},{"cell_type":"markdown","id":"012e958e-f600-4f05-97a2-096f1ce9c026","metadata":{"id":"012e958e-f600-4f05-97a2-096f1ce9c026"},"source":["## 1.4 transformacion flatmap\n","La transformacion flatMap , es parecida a la map, pero en este caso mapea y aplana.\n","Por ejemplo. Con flatmap, una linea con varias palabras, se podria mapear en varias listas. Es decir que de un elemento del RDD puede crear más de un elemento en el rdd nuevo.\n","\n","Dado el siguiente array de tres elementos\n","[\"Curso Especialista Inteligencia Artificial\",\"Desarrollo aplicaciones web\",\"Instituto IESAbastos es el mejor\" ]\n","\n","Si usamos map tendríamos 1 arrays con 3 elementos. Y dentro de cada array, de manera anidada tendriamos otro array con las palabras.\n","\n","[['Curso', 'Especialista', 'Inteligencia', 'Artificial'],\n"," ['Desarrollo', 'aplicaciones', 'web'],\n"," ['Instituto', 'IESAbastos', 'es', 'el', 'mejor']]\n","\n","Si usamos flatmap en vez de tener un array con 3 elementos, tendriamos un array con tantos elementos como palabras tenemos.\n","\n","\n","\n","**Ejercicio 3**, dadas las tres frases siguientes\n","\n","myLines = [\"Curso Especialista Inteligencia Artificial\",\"Desarrollo aplicaciones web\",\"Instituto IESAbastos es el mejor\" ]\n","\n","3.1 - Contruir un array que separe las frases en palabras usando map\n","\n","3.2 - construir un rdd que contenga tantas entradas como palabras tenemos en el array.\n","\n","\n"]},{"cell_type":"code","execution_count":5,"id":"4579a86c","metadata":{},"outputs":[{"data":{"text/plain":["['Curso Especialista Inteligencia Artificial',\n"," 'Desarrollo aplicaciones web',\n"," 'Instituto IESAbastos es el mejor']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["myLines = [\"Curso Especialista Inteligencia Artificial\", \"Desarrollo aplicaciones web\", \"Instituto IESAbastos es el mejor\"]\n","rddLines = spark.sparkContext.parallelize(myLines)\n","rddLines.collect()"]},{"cell_type":"code","execution_count":6,"id":"03c36bae-7f15-4780-b61a-9cb44a50a4e5","metadata":{"id":"03c36bae-7f15-4780-b61a-9cb44a50a4e5"},"outputs":[{"data":{"text/plain":["[['Curso', 'Especialista', 'Inteligencia', 'Artificial'],\n"," ['Desarrollo', 'aplicaciones', 'web'],\n"," ['Instituto', 'IESAbastos', 'es', 'el', 'mejor']]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["#ejercicio 3.1\n","rddWordsMap = rddLines.map(lambda line: line.split(\" \"))\n","rddWordsMap.collect()"]},{"cell_type":"code","execution_count":7,"id":"6bd2224f-177f-4550-b810-a8b482947558","metadata":{"id":"6bd2224f-177f-4550-b810-a8b482947558"},"outputs":[{"data":{"text/plain":["['Curso',\n"," 'Especialista',\n"," 'Inteligencia',\n"," 'Artificial',\n"," 'Desarrollo',\n"," 'aplicaciones',\n"," 'web',\n"," 'Instituto',\n"," 'IESAbastos',\n"," 'es',\n"," 'el',\n"," 'mejor']"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["#ejercicio 3.2\n","rddWordsFlatMap = rddLines.flatMap(lambda line: line.split(\" \"))\n","rddWordsFlatMap.collect()"]},{"cell_type":"markdown","id":"5c227546-c919-4586-822b-c32eac9b66cf","metadata":{"id":"5c227546-c919-4586-822b-c32eac9b66cf"},"source":["## 1.5.  Transformacion de Unión\n","\n","A través de esta transformación podemos unir dos RDDS\n","\n","**Ejercicio 4**: Coger dos de los RDDs anteriores y unirlos\n","\n","\n"]},{"cell_type":"code","execution_count":8,"id":"56fc7c4c-c426-4adc-802f-20f7a99e14f8","metadata":{"id":"56fc7c4c-c426-4adc-802f-20f7a99e14f8"},"outputs":[{"data":{"text/plain":["[['Curso', 'Especialista', 'Inteligencia', 'Artificial'],\n"," ['Desarrollo', 'aplicaciones', 'web'],\n"," ['Instituto', 'IESAbastos', 'es', 'el', 'mejor'],\n"," 'Curso',\n"," 'Especialista',\n"," 'Inteligencia',\n"," 'Artificial',\n"," 'Desarrollo',\n"," 'aplicaciones',\n"," 'web',\n"," 'Instituto',\n"," 'IESAbastos',\n"," 'es',\n"," 'el',\n"," 'mejor']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["#ejercio 4\n","\n","rddUnion = rddWordsMap.union(rddWordsFlatMap)\n","rddUnion.collect()"]},{"cell_type":"markdown","id":"83148bbd-4922-4360-9108-dde5749545e7","metadata":{"id":"83148bbd-4922-4360-9108-dde5749545e7"},"source":["## 1.6.  Transformacion Distinct\n","\n","Elimina todos los elementos repetidos de un RDD\n","\n","**Ejercicio 5**: Coger el RDD anterios (si tiene palabras repetidas) y aplicarle la transformacion distinct.\n","\n","Nota: En caso de no tener palabras repetidas usar el siguiente array\n","myarray = [\"haddoop\",\"bigdata\",\"haddoop\",\"spark\",\"data\", \"haddoop\",\"kibana\", \"RDD\", \"spark\",\"elastic\",\"bigdata\",\"IESAbastos\",\"IESAbastos\",\"IESAbastos\" ]\n","\n","\n"]},{"cell_type":"code","execution_count":9,"id":"1abbf541","metadata":{},"outputs":[{"data":{"text/plain":["['haddoop',\n"," 'bigdata',\n"," 'haddoop',\n"," 'spark',\n"," 'data',\n"," 'haddoop',\n"," 'kibana',\n"," 'RDD',\n"," 'spark',\n"," 'elastic',\n"," 'bigdata',\n"," 'IESAbastos',\n"," 'IESAbastos',\n"," 'IESAbastos']"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["myarray = [\"haddoop\",\"bigdata\",\"haddoop\",\"spark\",\"data\", \"haddoop\",\"kibana\", \"RDD\", \"spark\",\"elastic\",\"bigdata\",\"IESAbastos\",\"IESAbastos\",\"IESAbastos\" ]\n","rdd = spark.sparkContext.parallelize(myarray)\n","rdd.collect()"]},{"cell_type":"code","execution_count":10,"id":"e686d26c-ab49-4a6a-91bb-f3c002d4c05b","metadata":{"id":"e686d26c-ab49-4a6a-91bb-f3c002d4c05b"},"outputs":[{"data":{"text/plain":["['kibana',\n"," 'elastic',\n"," 'IESAbastos',\n"," 'haddoop',\n"," 'bigdata',\n"," 'spark',\n"," 'data',\n"," 'RDD']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["#Ejercicio 5\n","rddSinRepetidos = rdd.distinct()\n","rddSinRepetidos.collect()\n"]},{"cell_type":"markdown","id":"1b3895eb-1d86-4e51-9565-e1fcf69aac11","metadata":{"id":"1b3895eb-1d86-4e51-9565-e1fcf69aac11"},"source":["## 1.7.  Transformacion groupByKey y reduceByKey\n","\n","\n","groupBykey y reducebyKey permiten agrupar /o reducir el numero de elementos de una lista en funcion de la clave.  \n","Para ello:\n","- primero se tiene que crear una tupla clave y numero de veces que aparece la palabra.\n","\n","               (lambda x:  (x,1))\n","               \n","- una vez creada la tupla se aplica reduceByKey usando el operador add, lo que nos permite sumar el numero de veces que apacera un elemento\n","\n","**Ejercicio 6**: Contar el número de veces que aparecen todas las palabras del siguiente array\n","\n","myarray = [\"haddoop\",\"bigdata\",\"haddoop\",\"spark\",\"data\", \"haddoop\",\"kibana\", \"RDD\", \"spark\",\"elastic\",\"bigdata\",\"IESAbastos\",\"IESAbastos\",\"IESAbastos\" ]\n"]},{"cell_type":"code","execution_count":11,"id":"7ead9730","metadata":{},"outputs":[],"source":["myarray = [\"hadoop\",\"bigdata\",\"hadoop\",\"spark\",\"data\", \"hadoop\",\"kibana\", \"RDD\", \"spark\",\"elastic\",\"bigdata\",\"IESAbastos\",\"IESAbastos\",\"IESAbastos\"]\n","rdd = spark.sparkContext.parallelize(myarray)"]},{"cell_type":"code","execution_count":12,"id":"f332455d-1c67-474e-9c74-2c074c33cf3b","metadata":{"id":"f332455d-1c67-474e-9c74-2c074c33cf3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('kibana', 1), ('elastic', 1), ('IESAbastos', 3), ('bigdata', 2), ('spark', 2), ('hadoop', 3), ('data', 1), ('RDD', 1)]\n"]}],"source":["#Ejercicio 6\n","from operator import add   ## dejar esta linea\n","\n","# Creamos una tupla (palabra, 1) para cada palabra en el RDD\n","rddTuplas = rdd.map(lambda palabra: (palabra, 1))\n","\n","# Aplicamos reduceByKey para contar el número de veces que aparece cada palabra\n","rddConteo = rddTuplas.reduceByKey(lambda a, b: a + b)\n","\n","print(rddConteo.collect())"]},{"cell_type":"markdown","id":"6f071468-ebcc-4058-83b4-42bf0fd6f4de","metadata":{"id":"6f071468-ebcc-4058-83b4-42bf0fd6f4de"},"source":["# 1.8.  Transformacion ordenacion \"sortByKey\"\n","\n","Dada una lista de pares (clave ,valor), con el método sortbyKey podemos ordenar los valores por la clave, tanto de manera ascendente como descendente\n","\n","True --> ascendente\n","False--> descendente\n","\n","**Ejercicio 7**: Ordena los valores de la lista anterior por la clave  de manera ascendente"]},{"cell_type":"code","execution_count":13,"id":"1ff5e251-a751-4bba-93be-c801df37ee65","metadata":{"id":"1ff5e251-a751-4bba-93be-c801df37ee65"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('IESAbastos', 3), ('RDD', 1), ('bigdata', 2), ('data', 1), ('elastic', 1), ('hadoop', 3), ('kibana', 1), ('spark', 2)]\n"]}],"source":["#ejercio7\n","# Creamos un RDD a partir de la lista de tuplas (clave, valor)\n","listaTuplas = [(\"hadoop\", 3), (\"bigdata\", 2), (\"spark\", 2), (\"data\", 1), (\"kibana\", 1), (\"RDD\", 1), (\"elastic\", 1), (\"IESAbastos\", 3)]\n","rddTuplas = spark.sparkContext.parallelize(listaTuplas)\n","\n","# Utilizamos sortByKey para ordenar los valores por la clave en orden ascendente\n","rddOrdenadoAscendente = rddTuplas.sortByKey(ascending=True)\n","\n","print(rddOrdenadoAscendente.collect())\n"]},{"cell_type":"markdown","id":"068ed420-dade-48a3-9b20-2b5f10b698b5","metadata":{"id":"068ed420-dade-48a3-9b20-2b5f10b698b5"},"source":["<b>Ejercicio</b> 8  ordena los valores de la lista 6 por el valor en vez de por la clave"]},{"cell_type":"code","execution_count":14,"id":"b4e4a94c-a32f-45ef-b3aa-82d7a4b40312","metadata":{"id":"b4e4a94c-a32f-45ef-b3aa-82d7a4b40312"},"outputs":[{"data":{"text/plain":["[('data', 1),\n"," ('kibana', 1),\n"," ('RDD', 1),\n"," ('elastic', 1),\n"," ('bigdata', 2),\n"," ('spark', 2),\n"," ('hadoop', 3),\n"," ('IESAbastos', 3)]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["#ejercio 8\n","rddOrdenadoPorValor = rddTuplas.sortBy(lambda x: x[1], ascending=True)\n","rddOrdenadoPorValor.collect()"]},{"cell_type":"markdown","id":"fe637772-e1b5-4eb9-9ec4-6710d07e44eb","metadata":{"id":"fe637772-e1b5-4eb9-9ec4-6710d07e44eb"},"source":["Tambien es comun filtar las palabras que tengan un minimo numero de caracteres\n","\n","<b>Ejercicio 9</b> Dado un rdd de palabras filtrar las palabras que tienen una logitud mayor a x caracteres"]},{"cell_type":"code","execution_count":15,"id":"60886eca-d083-46db-addf-5c3a326741fd","metadata":{"id":"60886eca-d083-46db-addf-5c3a326741fd"},"outputs":[{"data":{"text/plain":["['bigdata', 'elastic', 'bigdata', 'IESAbastos', 'IESAbastos', 'IESAbastos']"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["#ejercio 9\n","rddFiltrado = rdd.filter(lambda palabra: len(palabra) > 6)\n","rddFiltrado.collect()"]},{"cell_type":"markdown","id":"7a4d776b-9733-43a3-84ed-d1f117ffc7cf","metadata":{"id":"7a4d776b-9733-43a3-84ed-d1f117ffc7cf"},"source":["\n","<b>Ejercicio 9</b> Dado el siguiente array\n","\n","myarraylineas = [\"haddoop bigdata haddoop spark tableau nodered\",\"data nodered  haddoop kibana RDD\", \"spark  elastic  bigdata IESAbastos IESAbastos IESAbastos\" ]\n","\n","Se pide mostrar las palablas de mas de 6 carácteres que aparecen en el array ordenadas de manera descendiente por el numero de veces que aparecen.\n","\n","\n"]},{"cell_type":"code","execution_count":16,"id":"98592279-7266-4b41-bb10-33a0ee6b8fc3","metadata":{"id":"98592279-7266-4b41-bb10-33a0ee6b8fc3"},"outputs":[],"source":["\n","myarraylineas = [\"hadoop bigdata haddoop spark tableau nodered\",\n","                 \"data nodered haddoop kibana RDD\",\n","                 \"spark elastic bigdata IESAbastos IESAbastos IESAbastos\"]\n","rddLineas = spark.sparkContext.parallelize(myarraylineas)\n","\n"]},{"cell_type":"code","execution_count":17,"id":"1c50ce6d","metadata":{},"outputs":[{"data":{"text/plain":["[('IESAbastos', 3),\n"," ('bigdata', 2),\n"," ('haddoop', 2),\n"," ('nodered', 2),\n"," ('tableau', 1),\n"," ('elastic', 1)]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["rddPalabras = rddLineas.flatMap(lambda linea: linea.split()) # Dividir líneas en palabras\n","rddPalabrasFiltradas = rddPalabras.filter(lambda palabra: len(palabra) > 6) # Filtrar las palabras con más de 6 caracteres\n","rddTuplas = rddPalabrasFiltradas.map(lambda palabra: (palabra, 1)) # Mapea la línea con una tupla\n","rddConteo = rddTuplas.reduceByKey(lambda a, b: a + b) # Cuenta el numero de veces que aparece cada palabra\n","rddOrdenadoDescendente = rddConteo.sortBy(lambda x: x[1], ascending=False) # Ordena en orden descendente\n","\n","rddOrdenadoDescendente.collect()"]},{"cell_type":"code","execution_count":18,"id":"c268a4e2","metadata":{},"outputs":[{"data":{"text/plain":["[('IESAbastos', 3),\n"," ('bigdata', 2),\n"," ('haddoop', 2),\n"," ('nodered', 2),\n"," ('tableau', 1),\n"," ('elastic', 1)]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["resultado = spark.sparkContext.parallelize(myarraylineas).flatMap(lambda linea: linea.split()).filter(lambda palabra: len(palabra) > 6).map(lambda palabra: (palabra, 1)).reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], ascending=False)\n","resultado.collect()"]},{"cell_type":"markdown","id":"cede1205","metadata":{},"source":["Ejercicio 10.\n","\n","Repetir el ejercicio, pero esta vez usad como entrada de datos el fichero txt que contiene la descripcion del quijote.\n","\n","Sacar por pantalla las 10 palabras de entre 6-10 carácteres que más se repitan en el libro\n","\n"]},{"cell_type":"code","execution_count":1,"id":"698546a0-7dfa-44b6-b0f7-6ec20bf7fd3c","metadata":{"id":"698546a0-7dfa-44b6-b0f7-6ec20bf7fd3c"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'quijote-es.txt'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquijote-es.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m archivo:\n\u001b[1;32m      2\u001b[0m     quijote \u001b[38;5;241m=\u001b[39m archivo\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n","File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'quijote-es.txt'"]}],"source":["with open(\"quijote-es.txt\", \"rb\") as archivo:\n","    quijote = archivo.read()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"Quijote\").getOrCreate()\n","\n","resultado = spark.sparkContext.parallelize(quijote).flatMap(lambda linea: linea.split()).filter(lambda palabra: len(palabra) > 6).map(lambda palabra: (palabra, 1)).reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], ascending=False)\n","resultado.collect()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":5}
