{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3323313-2482-42ed-8dbc-6542f7c3ed66",
   "metadata": {},
   "source": [
    "# RDD- Data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be7596-ebde-4edd-a60c-5d3f6e318ed9",
   "metadata": {},
   "source": [
    " # Conversions entre dataframe y RDD\n",
    " \n",
    " ## suponiendo que myDF es un dataframe, se usa la propiedad rdd para convertir de df a RDD\n",
    " \n",
    " myRDD=  myDF.rdd\n",
    " \n",
    " ## y desde Rdd podemos pasar a data frame mediante la funcion toDF()\n",
    " \n",
    " myDF2 = myRDD.toDF()\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5573f3e-2da6-4d5a-ba5e-7e5d9abb564c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/2019-Dec.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      9\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../2019-Dec.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmyFunc\u001b[39m(s):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrand\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mriche\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m s[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcart\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/2019-Dec.csv."
     ]
    }
   ],
   "source": [
    "#en python las funciones lambda tienen solo una instruccion\n",
    "#cuando queremos hacer varias instrucciones se puede usar una funcion\n",
    "\n",
    "from datetime import datetime, date\n",
    "#import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark3\").getOrCreate()\n",
    "\n",
    "df = spark.read.options(header='True', inferSchema='True').csv(\"../2019-Dec.csv\")\n",
    "\n",
    "def myFunc(s):\n",
    "    if s[\"brand\"]==\"riche\" and s[\"event_type\"]==\"cart\":\n",
    "        return [(s[\"product_id\"],1)]\n",
    "    return[]\n",
    "\n",
    "lines =df.rdd.flatMap(myFunc).reduceByKey(lambda a, b : a+b)\n",
    "for l in lines.collect():\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e2db56-2b85-44c3-8cdc-524e7e319215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|     _1| _2|\n",
      "+-------+---+\n",
      "|5842204|  2|\n",
      "|5842224|  2|\n",
      "|5842217|  2|\n",
      "|5842213|  2|\n",
      "|5842205|  2|\n",
      "|5842214|  2|\n",
      "|5842202|  4|\n",
      "|5846098| 10|\n",
      "|5842258|  4|\n",
      "|5842222|  2|\n",
      "|5842235|  1|\n",
      "|5842203|  1|\n",
      "|5842223|  1|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convertimos de RDD a DF\n",
    "myDF2 = lines.toDF()\n",
    "myDF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b202dde-3f06-4969-adff-f4572bd4ea7d",
   "metadata": {},
   "source": [
    "# Persistencia de los datos\n",
    "\n",
    "Una caracter√≠stica muy importante de Spark es que puede conservar el RDD/DF en la memoria. Cuando el RDD/DF se persiste, cada nodo conservar√° en memoria la partici√≥n del RDD en la que opera. Esto permite que operaciones posteriores sobre RDD no se tengan que calcular, ya que se hace uso de los datos en memoria\n",
    "\n",
    "<b>Uso normal de Spark sin persistencia</b>\n",
    "\n",
    "Dado el siguiente ejemplo, tras la construccion del RDD, se realizan varias acciones y transformaciones. Por cada accion se construye el RDD necesario para ejecutar la accion\n",
    "\n",
    "datos = sc.parallelize ([1,2,3,4,5,6,7,8,9,10])\n",
    "doble = datos.<b>map</b>(lambda x: x*2)\n",
    "\n",
    "print (\"la suma de los elementos x2 es %d\" <b> %doble.sum()</b>)\n",
    "\n",
    "menor6 = doble.<b>filter</b>(lambda x : x<6)\n",
    "\n",
    "print (\"Hax %d elementos menores que 6\" <b>menor6.count()</b>)\n",
    "\n",
    "\n",
    "Para grandes cantidades de datos, el recalcular varias veces una misma transformaci√≥n conllevar√° un sobrecoste nada despreciable en terminos de tiempo. \n",
    "Spark posee m√©todos para guardar las tranformaciones y evitar rec√°lculos\n",
    "\n",
    "* cache() --> cache guarda los datos en el nivel de almacenamiento por defecto \"memoria principal\"\n",
    "* persist() --> permite mediante un par√°metro, elegir un nivel de almacenamiento entre diferenters opciones tanto en memoria como en disco:\n",
    "\n",
    "    - MEMORY_ONLY ( COMPORTAMIENTO POR DEFECTO): Guarda los datos en memoria, los datos que no caben se recalculan cada vez.\n",
    "    - MEMORY_AND_DISK: Guarda los datos en memoria, los datos que no caben se guardan en disco.\n",
    "    - MEMORY_ONLY_2, MEMORY_AND_DISK_2: equivalente a las dos anteriores pero guarda los datos en 2 nodos.\n",
    "    - MEMORY_AND_DISK_SER  (SERIALIZADO)\n",
    "    - DISK_ONLY\n",
    "    - MEMORY_ONLY_SER  //SIMILAR A MEMORY ONLY, PERO OCUPA MENOS MEM, PROQUE SERIALIZA LOS OBJETOS\n",
    "    \n",
    "    ref: https://medium.com/iwannabedatadriven/spark-performace-cache-persist-iii-db4fa1bcc2c0\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb7385a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma de los elementos x2 es 110\n",
      "Hax 2 elementos menores que 6\n"
     ]
    }
   ],
   "source": [
    "#ejemplo sin persistencia\n",
    "datos = sc.parallelize ([1,2,3,4,5,6,7,8,9,10])\n",
    "doble = datos.map(lambda x: x*2)\n",
    "print (\"la suma de los elementos x2 es %d\" %doble.sum())\n",
    "menor6 = doble.filter(lambda x : x<6)\n",
    "print (\"Hax %d elementos menores que 6\" %menor6.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c0ffe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 4:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma de los elementos x2 es 110\n",
      "Hax 2 elementos menores que 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#ejemplo con persistencia\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "\n",
    "datos = sc.parallelize ([1,2,3,4,5,6,7,8,9,10])\n",
    "doble = datos.map(lambda x: x*2)\n",
    "doble.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "print (\"la suma de los elementos x2 es %d\" %doble.sum())\n",
    "menor6 = doble.filter(lambda x : x<6)\n",
    "print (\"Hax %d elementos menores que 6\" %menor6.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d719ced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|      event_type|\n",
      "+----------------+\n",
      "|        purchase|\n",
      "|            view|\n",
      "|            cart|\n",
      "|remove_from_cart|\n",
      "+----------------+\n",
      "\n",
      "6.5876171588897705\n"
     ]
    }
   ],
   "source": [
    "#ejemplo tienda virtual\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark4\").getOrCreate()\n",
    "t1= time.time()\n",
    "\n",
    "\n",
    "df = spark.read.options(header='True', inferSchema='True').csv(\"../2019-Dec.csv\")\n",
    "df.unpersist()\n",
    "df.count()\n",
    "df.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "df2=df.filter(\"event_type='cart'\").filter(\"brand='runail'\").head(2)\n",
    "df.select(\"event_type\").distinct().show()\n",
    "\n",
    "\n",
    "t2= time.time()\n",
    "\n",
    "dif = t2-t1\n",
    "print (dif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87513957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fb82873-7abc-4e51-a2f1-54c93d1e2b1d",
   "metadata": {},
   "source": [
    "  # broadcast y Acumuladores de spark\n",
    "   \n",
    " http://sparkbyexamples.com/spark/spark-broadcast-variables\n",
    " \n",
    "## Broadcast (variables globales de solo lectura)\n",
    "las variables de tipo broadcast son variables de solo lectura que se pueden consultar en todos los nodos\n",
    " \n",
    "## Acumuladores  (varialbles globales que se pueden modificas)\n",
    "las variables globales tradicionales no funcionan en Spark, se tuenen que usar acumuladores\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb4bdd9d-2afb-4c0d-8de9-d9786d6bbcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "broadcast = spark.sparkContext.broadcast([1,2,3])\n",
    "print (broadcast.value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "974f8de7-1492-49e7-9539-2b1118478900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "accum =  spark.sparkContext.accumulator (0)\n",
    "sumatorioTradicional =0\n",
    "\n",
    "def suma(x):\n",
    "    global sumatorioTradicional\n",
    "    \n",
    "    accum.add(x)\n",
    "    sumatorioTradicional+x\n",
    "    \n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6])\n",
    "rdd.foreach(suma)\n",
    "\n",
    "print (accum)\n",
    "print (sumatorioTradicional)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077484e-c065-4006-8130-262a95c47601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
