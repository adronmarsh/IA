{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3323313-2482-42ed-8dbc-6542f7c3ed66",
   "metadata": {},
   "source": [
    "# RDD- Data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be7596-ebde-4edd-a60c-5d3f6e318ed9",
   "metadata": {},
   "source": [
    " # Conversions entre dataframe y RDD\n",
    " \n",
    " ## suponiendo que myDF es un dataframe, se usa la propiedad rdd para convertir de df a RDD\n",
    " \n",
    " myRDD=  myDF.rdd\n",
    " \n",
    " ## y desde Rdd podemos pasar a data frame mediante la funcion toDF()\n",
    " \n",
    " myDF2 = myRDD.toDF()\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5573f3e-2da6-4d5a-ba5e-7e5d9abb564c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/2019-Dec.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      9\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../2019-Dec.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmyFunc\u001b[39m(s):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrand\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mriche\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m s[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcart\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/2019-Dec.csv."
     ]
    }
   ],
   "source": [
    "#en python las funciones lambda tienen solo una instruccion\n",
    "#cuando queremos hacer varias instrucciones se puede usar una funcion\n",
    "\n",
    "from datetime import datetime, date\n",
    "#import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark3\").getOrCreate()\n",
    "\n",
    "df = spark.read.options(header='True', inferSchema='True').csv(\"../2019-Dec.csv\")\n",
    "\n",
    "def myFunc(s):\n",
    "    if s[\"brand\"]==\"riche\" and s[\"event_type\"]==\"cart\":\n",
    "        return [(s[\"product_id\"],1)]\n",
    "    return[]\n",
    "\n",
    "lines =df.rdd.flatMap(myFunc).reduceByKey(lambda a, b : a+b)\n",
    "for l in lines.collect():\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e2db56-2b85-44c3-8cdc-524e7e319215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|     _1| _2|\n",
      "+-------+---+\n",
      "|5842204|  2|\n",
      "|5842224|  2|\n",
      "|5842217|  2|\n",
      "|5842213|  2|\n",
      "|5842205|  2|\n",
      "|5842214|  2|\n",
      "|5842202|  4|\n",
      "|5846098| 10|\n",
      "|5842258|  4|\n",
      "|5842222|  2|\n",
      "|5842235|  1|\n",
      "|5842203|  1|\n",
      "|5842223|  1|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convertimos de RDD a DF\n",
    "myDF2 = lines.toDF()\n",
    "myDF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b202dde-3f06-4969-adff-f4572bd4ea7d",
   "metadata": {},
   "source": [
    "# Persistencia de los datos\n",
    "\n",
    "Una característica muy importante de Spark es que puede conservar el RDD/DF en la memoria. Cuando el RDD/DF se persiste, cada nodo conservará en memoria la partición del RDD en la que opera. Esto permite que operaciones posteriores sobre RDD no se tengan que calcular, ya que se hace uso de los datos en memoria\n",
    "\n",
    "<b>Uso normal de Spark sin persistencia</b>\n",
    "\n",
    "Dado el siguiente ejemplo, tras la construccion del RDD, se realizan varias acciones y transformaciones. Por cada accion se construye el RDD necesario para ejecutar la accion\n",
    "\n",
    "datos = sc.parallelize ([1,2,3,4,5,6,7,8,9,10])\n",
    "doble = datos.<b>map</b>(lambda x: x*2)\n",
    "\n",
    "print (\"la suma de los elementos x2 es %d\" <b> %doble.sum()</b>)\n",
    "\n",
    "menor6 = doble.<b>filter</b>(lambda x : x<6)\n",
    "\n",
    "print (\"Hax %d elementos menores que 6\" <b>menor6.count()</b>)\n",
    "\n",
    "\n",
    "Para grandes cantidades de datos, el recalcular varias veces una misma transformación conllevará un sobrecoste nada despreciable en terminos de tiempo. \n",
    "Spark posee métodos para guardar las tranformaciones y evitar recálculos\n",
    "\n",
    "* cache() --> cache guarda los datos en el nivel de almacenamiento por defecto \"memoria principal\"\n",
    "* persist() --> permite mediante un parámetro, elegir un nivel de almacenamiento entre diferenters opciones tanto en memoria como en disco:\n",
    "\n",
    "    - MEMORY_ONLY ( COMPORTAMIENTO POR DEFECTO): Guarda los datos en memoria, los datos que no caben se recalculan cada vez.\n",
    "    - MEMORY_AND_DISK: Guarda los datos en memoria, los datos que no caben se guardan en disco.\n",
    "    - MEMORY_ONLY_2, MEMORY_AND_DISK_2: equivalente a las dos anteriores pero guarda los datos en 2 nodos.\n",
    "    - MEMORY_AND_DISK_SER  (SERIALIZADO)\n",
    "    - DISK_ONLY\n",
    "    - MEMORY_ONLY_SER  //SIMILAR A MEMORY ONLY, PERO OCUPA MENOS MEM, PROQUE SERIALIZA LOS OBJETOS\n",
    "    \n",
    "    ref: https://medium.com/iwannabedatadriven/spark-performace-cache-persist-iii-db4fa1bcc2c0\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb7385a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma de los elementos x2 es 110\n",
      "Hax 2 elementos menores que 6\n"
     ]
    }
   ],
   "source": [
    "#ejemplo sin persistencia\n",
    "datos = sc.parallelize ([1,2,3,4,5,6,7,8,9,10])\n",
    "doble = datos.map(lambda x: x*2)\n",
    "print (\"la suma de los elementos x2 es %d\" %doble.sum())\n",
    "menor6 = doble.filter(lambda x : x<6)\n",
    "print (\"Hax %d elementos menores que 6\" %menor6.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c0ffe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 4:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma de los elementos x2 es 110\n",
      "Hax 2 elementos menores que 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#ejemplo con persistencia\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "\n",
    "datos = sc.parallelize ([1,2,3,4,5,6,7,8,9,10])\n",
    "doble = datos.map(lambda x: x*2)\n",
    "doble.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "print (\"la suma de los elementos x2 es %d\" %doble.sum())\n",
    "menor6 = doble.filter(lambda x : x<6)\n",
    "print (\"Hax %d elementos menores que 6\" %menor6.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d719ced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|      event_type|\n",
      "+----------------+\n",
      "|        purchase|\n",
      "|            view|\n",
      "|            cart|\n",
      "|remove_from_cart|\n",
      "+----------------+\n",
      "\n",
      "6.5876171588897705\n"
     ]
    }
   ],
   "source": [
    "#ejemplo tienda virtual\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark4\").getOrCreate()\n",
    "t1= time.time()\n",
    "\n",
    "\n",
    "df = spark.read.options(header='True', inferSchema='True').csv(\"../2019-Dec.csv\")\n",
    "df.unpersist()\n",
    "df.count()\n",
    "df.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "df2=df.filter(\"event_type='cart'\").filter(\"brand='runail'\").head(2)\n",
    "df.select(\"event_type\").distinct().show()\n",
    "\n",
    "\n",
    "t2= time.time()\n",
    "\n",
    "dif = t2-t1\n",
    "print (dif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87513957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fb82873-7abc-4e51-a2f1-54c93d1e2b1d",
   "metadata": {},
   "source": [
    "  # broadcast y Acumuladores de spark\n",
    "   \n",
    " http://sparkbyexamples.com/spark/spark-broadcast-variables\n",
    " \n",
    "## Broadcast (variables globales de solo lectura)\n",
    "las variables de tipo broadcast son variables de solo lectura que se pueden consultar en todos los nodos\n",
    " \n",
    "## Acumuladores  (varialbles globales que se pueden modificas)\n",
    "las variables globales tradicionales no funcionan en Spark, se tuenen que usar acumuladores\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb4bdd9d-2afb-4c0d-8de9-d9786d6bbcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "broadcast = spark.sparkContext.broadcast([1,2,3])\n",
    "print (broadcast.value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "974f8de7-1492-49e7-9539-2b1118478900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "accum =  spark.sparkContext.accumulator (0)\n",
    "sumatorioTradicional =0\n",
    "\n",
    "def suma(x):\n",
    "    global sumatorioTradicional\n",
    "    \n",
    "    accum.add(x)\n",
    "    sumatorioTradicional+x\n",
    "    \n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6])\n",
    "rdd.foreach(suma)\n",
    "\n",
    "print (accum)\n",
    "print (sumatorioTradicional)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077484e-c065-4006-8130-262a95c47601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
