{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3323313-2482-42ed-8dbc-6542f7c3ed66",
   "metadata": {},
   "source": [
    "# RDD- Data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be7596-ebde-4edd-a60c-5d3f6e318ed9",
   "metadata": {},
   "source": [
    " # Conversions entre dataframe y RDD\n",
    " \n",
    " ## suponiendo que myDF es un dataframe, se usa la propiedad rdd para convertir de df a RDD\n",
    " \n",
    " myRDD=  myDF.rdd\n",
    " \n",
    " ## y desde Rdd podemos pasar a data frame mediante la funcion toDF()\n",
    " \n",
    " myDF2 = myRDD.toDF()\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5573f3e-2da6-4d5a-ba5e-7e5d9abb564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5842204, 2)\n",
      "(5842224, 2)\n",
      "(5842217, 2)\n",
      "(5842213, 2)\n",
      "(5842205, 2)\n",
      "(5842214, 2)\n",
      "(5842202, 4)\n",
      "(5846098, 10)\n",
      "(5842258, 4)\n",
      "(5842222, 2)\n",
      "(5842235, 1)\n",
      "(5842203, 1)\n",
      "(5842223, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#en python las funciones lambda tienen solo una instruccion\n",
    "#cuando queremos hacer varias instrucciones se puede usar una funcion\n",
    "\n",
    "from datetime import datetime, date\n",
    "#import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.read.options(header='True', inferSchema='True').csv(\"../2019-Dec.csv\")\n",
    "\n",
    "\n",
    "def myFunc(s):\n",
    "    if s[\"brand\"]==\"riche\" and s[\"event_type\"]==\"cart\":\n",
    "        return [(s[\"product_id\"],1)]\n",
    "    return[]\n",
    "\n",
    "lines =df.rdd.flatMap(myFunc).reduceByKey(lambda a, b : a+b)\n",
    "for l in lines.collect():\n",
    "    print(l)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e2db56-2b85-44c3-8cdc-524e7e319215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|     _1| _2|\n",
      "+-------+---+\n",
      "|5842204|  2|\n",
      "|5842224|  2|\n",
      "|5842217|  2|\n",
      "|5842213|  2|\n",
      "|5842205|  2|\n",
      "|5842214|  2|\n",
      "|5842202|  4|\n",
      "|5846098| 10|\n",
      "|5842258|  4|\n",
      "|5842222|  2|\n",
      "|5842235|  1|\n",
      "|5842203|  1|\n",
      "|5842223|  1|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convertimos de RDD a DF\n",
    "myDF2 = lines.toDF()\n",
    "myDF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b202dde-3f06-4969-adff-f4572bd4ea7d",
   "metadata": {},
   "source": [
    "# Persistencia de los datos\n",
    "\n",
    "Una característica muy importante de Spark es que puede conservar el RDD/DF en la memoria. Cuando el RDD/DF se persiste, cada nodo conservará en memoria la partición del RDD en la que opera. Esto permite que operaciones posteriores sobre RDD no se tengan que calcular, ya que se hace uso de los datos en memoria\n",
    "\n",
    "<b>Uso normal de Spark sin persistencia</b>\n",
    "\n",
    "Dado el siguiente ejemplo, tras la construccion del RDD, se realizan varias acciones y transformaciones. Por cada accion se construye el RDD necesario para ejecutar la accion\n",
    "\n",
    "datos = sc.parallelize ([1,2,3,4,5,6,7,8,9,10])\n",
    "doble = datos.<b>map</b>(lambda x: x*2)\n",
    "\n",
    "print (\"la suma de los elementos x2 es %d\" <b> %doble.sum()</b>)\n",
    "\n",
    "menor6 = doble.<b>filter</b>(lambda x : x<6)\n",
    "\n",
    "print (\"Hax %d elementos menores que 6\" <b>menor6.count()</b>)\n",
    "\n",
    "\n",
    "Para grandes cantidades de datos, el recalcular varias veces una misma transformación conllevará un sobrecoste nada despreciable en terminos de tiempo. \n",
    "Spark posee métodos para guardar las tranformaciones y evitar recálculos\n",
    "\n",
    "* cache() --> cache guarda los datos en el nivel de almacenamiento por defecto \"memoria principal\"\n",
    "* persist() --> permite mediante un parámetro, elegir un nivel de almacenamiento entre diferenters opciones tanto en memoria como en disco:\n",
    "\n",
    "    - MEMORY_ONLY ( COMPORTAMIENTO POR DEFECTO): Guarda los datos en memoria, los datos que no caben se recalculan cada vez.\n",
    "    - MEMORY_AND_DISK: Guarda los datos en memoria, los datos que no caben se guardan en disco.\n",
    "    - MEMORY_ONLY_2, MEMORY_AND_DISK_2: equivalente a las dos anteriores pero guarda los datos en 2 nodos.\n",
    "    - MEMORY_AND_DISK_SER  (SERIALIZADO)\n",
    "    - DISK_ONLY\n",
    "    - MEMORY_ONLY_SER  //SIMILAR A MEMORY ONLY, PERO OCUPA MENOS MEM, PROQUE SERIALIZA LOS OBJETOS\n",
    "    \n",
    "    ref: https://medium.com/iwannabedatadriven/spark-performace-cache-persist-iii-db4fa1bcc2c0\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb7385a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma de los elementos x2 es 110\n",
      "Hax 2 elementos menores que 6\n"
     ]
    }
   ],
   "source": [
    "#ejemplo sin persistencia\n",
    "datos = sc.parallelize ([1,2,3,4,5,6,7,8,9,10])\n",
    "doble = datos.map(lambda x: x*2)\n",
    "print (\"la suma de los elementos x2 es %d\" %doble.sum())\n",
    "menor6 = doble.filter(lambda x : x<6)\n",
    "print (\"Hax %d elementos menores que 6\" %menor6.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c0ffe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma de los elementos x2 es 110\n",
      "Hax 2 elementos menores que 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#ejemplo con persistencia\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "\n",
    "datos = sc.parallelize ([1,2,3,4,5,6,7,8,9,10])\n",
    "doble = datos.map(lambda x: x*2)\n",
    "doble.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "print (\"la suma de los elementos x2 es %d\" %doble.sum())\n",
    "menor6 = doble.filter(lambda x : x<6)\n",
    "print (\"Hax %d elementos menores que 6\" %menor6.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d719ced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|      event_type|\n",
      "+----------------+\n",
      "|        purchase|\n",
      "|            view|\n",
      "|            cart|\n",
      "|remove_from_cart|\n",
      "+----------------+\n",
      "\n",
      "6.5876171588897705\n"
     ]
    }
   ],
   "source": [
    "#ejemplo tienda virtual\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark4\").getOrCreate()\n",
    "t1= time.time()\n",
    "\n",
    "\n",
    "df = spark.read.options(header='True', inferSchema='True').csv(\"../2019-Dec.csv\")\n",
    "df.unpersist()\n",
    "df.count()\n",
    "df.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "df2=df.filter(\"event_type='cart'\").filter(\"brand='runail'\").head(2)\n",
    "df.select(\"event_type\").distinct().show()\n",
    "\n",
    "\n",
    "t2= time.time()\n",
    "\n",
    "dif = t2-t1\n",
    "print (dif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87513957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fb82873-7abc-4e51-a2f1-54c93d1e2b1d",
   "metadata": {},
   "source": [
    "  # broadcast y Acumuladores de spark\n",
    "   \n",
    " http://sparkbyexamples.com/spark/spark-broadcast-variables\n",
    " \n",
    "## Broadcast (variables globales de solo lectura)\n",
    "las variables de tipo broadcast son variables de solo lectura que se pueden consultar en todos los nodos\n",
    " \n",
    "## Acumuladores  (varialbles globales que se pueden modificas)\n",
    "las variables globales tradicionales no funcionan en Spark, se tuenen que usar acumuladores\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb4bdd9d-2afb-4c0d-8de9-d9786d6bbcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "broadcast = spark.sparkContext.broadcast([1,2,3])\n",
    "print (broadcast.value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "974f8de7-1492-49e7-9539-2b1118478900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "accum =  spark.sparkContext.accumulator (0)\n",
    "sumatorioTradicional =0\n",
    "\n",
    "def suma(x):\n",
    "    global sumatorioTradicional\n",
    "    \n",
    "    accum.add(x)\n",
    "    sumatorioTradicional+x\n",
    "    \n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6])\n",
    "rdd.foreach(suma)\n",
    "\n",
    "print (accum)\n",
    "print (sumatorioTradicional)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077484e-c065-4006-8130-262a95c47601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
