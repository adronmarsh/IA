{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ca568a-5dcf-4cff-8298-6d1232e217b5",
   "metadata": {},
   "source": [
    "# 3 Spark DataFrame y Spark SQL\n",
    "\n",
    "\n",
    "\n",
    "### como se creaba un RDD\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark1\").getOrCreate()\n",
    "\n",
    "<b>rdd = spark.sparkContext.parallelize([1,2,3,4,5,6])\n",
    "</b>\n",
    "\n",
    "## como creamos un Dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark3\").getOrCreate()\n",
    "\n",
    "<b>df = spark.read.options(header='True', inferSchema='True').csv(\"./datos/2019-Nov.csv\")\n",
    "</b>\n",
    "\n",
    "\n",
    "\n",
    "- ejemplo de Data Frame y Spark SQL con python\n",
    "\n",
    "\n",
    "- Descargamos los datos de una tienda de cosmetico 2,5GB partidos en 5 ficheros \n",
    "https://www.kaggle.com/mkechinov/ecommerce-events-history-in-cosmetics-shop\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59c5373-1be8-479e-96a8-f587471b3af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f0218389ed0>\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/datos/*.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(spark)\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./datos/*.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/jovyan/datos/*.csv."
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark3\").getOrCreate()\n",
    "print(spark)\n",
    "\n",
    "df = spark.read.options(header='True', inferSchema='True').csv(\"./datos/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a203caf-909b-4e80-9768-69eeb57f30e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#contamos el numero de registros\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55149a-52d0-4ec7-b048-c7ebfba4c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sacamos la estructura de los datos\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b63a3-5add-4ff2-a96a-f307d5be3442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mostramos los primeros elementos\n",
    "#filter , head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c3527d-01a4-41c9-a3b4-ffc21d235d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buscamos todos los tipos de eventos\n",
    "#select, distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec08be1-f0e2-4e2c-a7b5-0dc896f1df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buscamos todas las marcas distintas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e91b57-f107-436b-bbce-b77e80d5891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buscamos todas sesiones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b939a-8f5a-4672-8171-083ee83749dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#al ser una tienda y tener listado de compras, podemos sacar listados de produyctos relacionados\n",
    "\n",
    "#primero scamos dotos los podructos que se hayan comprado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f116cff9-5353-4caa-b746-b0409de46579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dado el producto 5844397\n",
    "# obtengo todas las sesiones de compra que contengan dicho producto\n",
    "\n",
    "#buscamos los procutos que se hayan comprado en dichas sesiones\n",
    "#isin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935e3aa-5384-4dcd-b2b9-2dafb966a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#products_related.write.mode(\"overwrite\").csv(\"./datos/products_related\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ea2a6-011e-4376-b0a9-0d773a329938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtener las marcas que mas se meten en el carrito\n",
    "#marcas= df.filter(\"event_type='cart'\").groupBy(\"brand\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14392e1-73af-43e7-a5a5-6297fb21174d",
   "metadata": {},
   "source": [
    "# Con Data frame tambien podemos trabajar con la api de SQL\n",
    "\n",
    "Las consultas SQL no funcionan con RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4177d76-ebca-4066-b1c5-2e9e4f062bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "spark.sql(\"select * from data limit 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189177b6-052e-4602-92f6-d84c6873b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from data where event_type='cart' limit 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c657177a-333d-4864-82f8-def93bfcaecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from data where event_type='cart'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5dbf0c-cff7-496f-adeb-a29a2643bdec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc6c294-31e4-447b-afc3-f0cffa0b0ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
