{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiDzBoKGwmMZ"
      },
      "source": [
        "Entrenamiento redes neuronales con Keras\n",
        "\n",
        "En esta primera parte, vamos a utilizar una red neuronal para clasificar imágenes de prendas de ropa. Para ello, utilizaremos Keras con TensorFlow.\n",
        "\n",
        "El dataset a utilizar es Fashion MNIST, un problema sencillo con imágenes pequeñas de ropa, pero más interesante que el dataset de MNIST. Puedes consultar más información sobre el dataset en [este enlace](https://github.com/zalandoresearch/fashion-mnist).\n",
        "\n",
        "El código utilizado para contestar tiene que quedar claramente reflejado en el Notebook. Puedes crear nuevas cells si así lo deseas para estructurar tu código y sus salidas. A la hora de entregar el notebook, **asegúrate de que los resultados de ejecutar tu código han quedado guardados** (por ejemplo, a la hora de entrenar una red neuronal tiene que verse claramente un log de los resultados de cada epoch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSHr268SwmMa"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "\n",
        "from keras.optimizers import RMSprop, SGD\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhYq8-CvwmMc"
      },
      "source": [
        "Primero, vamos a obtener los datos. Por suerte para nosotros, estos pueden ser descargados directamente desde Keras, por lo que no tendremos que preocuparnos de tratar con ficheros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mjrFULOwmMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f11807-c68f-4356-f4bd-ebb7e630b7f1"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOVoeoj0wmMe"
      },
      "source": [
        "Acto seguido, normalizamos esos datos de manera similar a como hemos visto con MNIST, obteniendo valores entre 0 y 1. Este paso es muy importante para el correcto funcionamiento de nuestra red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EgKN2YWwmMf"
      },
      "source": [
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaqXlSMBwmMg"
      },
      "source": [
        "## 1. Información sobre el dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0aer8ZZwmMh"
      },
      "source": [
        "Una vez tenemos los datos cargados en memoria, vamos a obtener información sobre los mismos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-im9PnEwmMh"
      },
      "source": [
        "**Pregunta 1.1 ** ¿Cuántas imágenes hay de *training* y de *test*? ¿Qué tamaño tienen las imágenes?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvP0Y4SCwmMi",
        "outputId": "6dce4bb5-f247-4f42-b904-a95f84741f0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "print(\"Las imagenes de training son: \" + str(len(x_train)))\n",
        "print(\"Las imagenes de test son: \" + str(len(x_test)))\n",
        "print('Dimensiones x_train: ', np.shape(x_train))\n",
        "print('Tamaño dimensión 1 x_train: ', np.size(x_train, 1))\n",
        "print('Tamaño dimensión 2 x_train: ', np.size(x_train, 2))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Las imagenes de training son: 60000\n",
            "Las imagenes de test son: 10000\n",
            "Dimensiones x_train:  (60000, 28, 28)\n",
            "Tamaño dimensión 1 x_train:  28\n",
            "Tamaño dimensión 2 x_train:  28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90AjAOx5O5XX"
      },
      "source": [
        " \n",
        " * Hay 60000 imagenes de training.\n",
        " * Hay 10000 imagenes de entrenamiento.\n",
        " * Cada imagen  esta formada por pixeles de 28x28.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2LsvfHOwmMk"
      },
      "source": [
        "**Pregunta 1.2 ** Realizar una exploración de las variables que contienen los datos. Describir en qué consiste un example del dataset (qué información se guarda en cada imagen) y describir qué contiene la información en y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W5rzaGxwmMk",
        "outputId": "3919b50b-f592-4296-9637-5dee10747eab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Dimensiones y_train: ', np.shape(y_train))\n",
        "print('Elementos de y_train: ', np.unique(y_train))\n",
        "y_train[0]\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones y_train:  (60000,)\n",
            "Elementos de y_train:  [0 1 2 3 4 5 6 7 8 9]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaEWKFyvwmMm"
      },
      "source": [
        " * Cada imagen del conjunto de entranamiento esta formada por pixeles de 28x28. Con valores de pixel que varian de 0 a 255. Como hemos normalizado a principio de la práctica ahora varia de 0 a 1.\n",
        " * La y es un vector de enteros, que van del 0 al 9. Estos corresponden a la class de ropa que la imagen representa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPNa3nH0wmMn"
      },
      "source": [
        "[texto del enlace](https://)Vamos a **visualizar** una imagen de ejemplo. Prueba tu mismo a cambiar la imagen en uso para explorar el dataset visualmente ejecutando el siguiente código:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlQx9uk3wmMn"
      },
      "source": [
        "def visualize_example(x):\n",
        "    plt.figure()\n",
        "    plt.imshow(x)\n",
        "    plt.colorbar()\n",
        "    plt.grid(False)\n",
        "    plt.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTHt4fd2wmMp",
        "outputId": "4c131547-53e2-42dd-c908-4182a116d4aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "visualize_example(x_train[11])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USVefNOIwmMt"
      },
      "source": [
        "## 2. Entrenamiento de una red neuronal simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSWjUeOdwmMt"
      },
      "source": [
        "**Pregunta 2 **. Utilizando Keras, y preparando los datos de X e y como fuera necesario, define y entrena una red neuronal que sea capaz de clasificar imágenes de Fashion MNIST con las siguientes características:\n",
        "\n",
        "* Dos hidden layers de tamaños 128 y 64, utilizando unidades **sigmoid**\n",
        "* Optimizador **sgd**.\n",
        "* Durante el entrenamiento, la red tiene que mostrar resultados de **loss** y **accuracy** por cada epoch.\n",
        "* La red debe entrenar durante **20 epochs** y batch size de **64**.\n",
        "* La última capa debe de ser una capa **softmax**.\n",
        "\n",
        "Tu red tendría que ser capaz de superar fácilmente 60% de accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVTsBpSHcg-A",
        "outputId": "dafd0d19-555e-4411-b383-63d946e04b68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_train"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCh7qRMTwmMt"
      },
      "source": [
        "#Tenemos 10 clases que varian de 0 a 9 como hemos visto en los puntos anteriores.\n",
        "num_clases=10\n",
        "\n",
        "#Transformamos el formato de las imagenes de un vector bi-dimensional (de 28 por 28 pixeles) a un vector unidimensional (de 28*28 pixeles = 784 pixeles)\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "\n",
        "\n",
        "#Transformamos el vector de la variable y es categorico.\n",
        "y_train = keras.utils.to_categorical(y_train, num_clases)\n",
        "y_test = keras.utils.to_categorical(y_test, num_clases)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk8G7A7gcd66",
        "outputId": "345e7a6c-cc3a-40ae-e01c-17d0cac61f8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128,activation='sigmoid', input_shape=(784,)))\n",
        "model.add(Dense(64,activation='sigmoid'))\n",
        "model.add(Dense(num_clases,activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,386\n",
            "Trainable params: 109,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwIDAxcVgseJ"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=SGD(),\n",
        "             \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PZMWZzdiL-6",
        "outputId": "fa65b5c0-3889-466c-9407-fdbbdb67d6e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_size= 64\n",
        "epochs = 20\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size = batch_size,\n",
        "                    epochs = epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "938/938 [==============================] - 20s 19ms/step - loss: 2.1809 - accuracy: 0.3876 - val_loss: 2.0098 - val_accuracy: 0.5374\n",
            "Epoch 2/20\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 1.7935 - accuracy: 0.5429 - val_loss: 1.5892 - val_accuracy: 0.5617\n",
            "Epoch 3/20\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 1.4311 - accuracy: 0.6116 - val_loss: 1.2999 - val_accuracy: 0.6166\n",
            "Epoch 4/20\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 1.1982 - accuracy: 0.6539 - val_loss: 1.1188 - val_accuracy: 0.6596\n",
            "Epoch 5/20\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 1.0482 - accuracy: 0.6823 - val_loss: 0.9970 - val_accuracy: 0.6854\n",
            "Epoch 6/20\n",
            "938/938 [==============================] - 6s 7ms/step - loss: 0.9432 - accuracy: 0.7035 - val_loss: 0.9092 - val_accuracy: 0.7039\n",
            "Epoch 7/20\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.8653 - accuracy: 0.7183 - val_loss: 0.8437 - val_accuracy: 0.7134\n",
            "Epoch 8/20\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.8062 - accuracy: 0.7284 - val_loss: 0.7931 - val_accuracy: 0.7266\n",
            "Epoch 9/20\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.7610 - accuracy: 0.7382 - val_loss: 0.7546 - val_accuracy: 0.7345\n",
            "Epoch 10/20\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.7254 - accuracy: 0.7457 - val_loss: 0.7234 - val_accuracy: 0.7400\n",
            "Epoch 11/20\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.6969 - accuracy: 0.7528 - val_loss: 0.7001 - val_accuracy: 0.7486\n",
            "Epoch 12/20\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.6733 - accuracy: 0.7593 - val_loss: 0.6783 - val_accuracy: 0.7547\n",
            "Epoch 13/20\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.6534 - accuracy: 0.7649 - val_loss: 0.6618 - val_accuracy: 0.7549\n",
            "Epoch 14/20\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.6363 - accuracy: 0.7706 - val_loss: 0.6455 - val_accuracy: 0.7650\n",
            "Epoch 15/20\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.6210 - accuracy: 0.7758 - val_loss: 0.6326 - val_accuracy: 0.7683\n",
            "Epoch 16/20\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.6076 - accuracy: 0.7803 - val_loss: 0.6200 - val_accuracy: 0.7761\n",
            "Epoch 17/20\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.5953 - accuracy: 0.7853 - val_loss: 0.6094 - val_accuracy: 0.7799\n",
            "Epoch 18/20\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.5840 - accuracy: 0.7898 - val_loss: 0.5997 - val_accuracy: 0.7826\n",
            "Epoch 19/20\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.5738 - accuracy: 0.7933 - val_loss: 0.5897 - val_accuracy: 0.7873\n",
            "Epoch 20/20\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.5642 - accuracy: 0.7969 - val_loss: 0.5820 - val_accuracy: 0.7871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZux6TmvwmMv"
      },
      "source": [
        "## 3. Evaluación del modelo en datos de test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SzHBkzrwmMw"
      },
      "source": [
        "Una vez hemos entrenado nuestro modelo, vamos a evaluarlo en los datos de test de Fashion MNIST.\n",
        "\n",
        "**Pregunta 3.1 **. Utilizando el modelo recién entrenado, obtener la accuracy resultante en el dataset de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CSLZkW1wmMw",
        "outputId": "9aff1951-c63a-4a88-b89e-74906ba9602b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluation = model.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)  \n",
        "evaluation"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.5820 - accuracy: 0.7871\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5820273160934448, 0.7871000170707703]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud0JCcZpwmMx"
      },
      "source": [
        "**Pregunta 3.2 **. Utilizando el método **predict** de Keras, realizar predicciones para los datos de test. Por cada predicción resultante, ¿qué significan los números que obtenemos al hacer predict? ¿Cómo podemos obtener el valor de la clase resultante? (recordar que estamos utilizando una capa softmax para clasificar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wrXVJJ9wmMy",
        "outputId": "f1925f79-c8ad-44a2-9d47-f81b3343c4b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Ejemplo imagen 0\n",
        "predictions = model.predict(x_test)\n",
        "#Imprimimos las probabilidades de que pertenezca a cada clase.\n",
        "print(predictions[0])\n",
        "#Obtenemos el valor mayor para saber la clase resultante.\n",
        "print(np.argmax(predictions[0]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step\n",
            "[4.0831823e-05 2.9106006e-05 3.7726457e-04 4.2516703e-04 1.2588150e-04\n",
            " 2.0985165e-01 1.8616822e-04 2.2510168e-01 1.4942086e-02 5.4892015e-01]\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZynBpMBwmM2"
      },
      "source": [
        "Al ser la última capa de tipo softmax, el resultado es la probabilidad que la imagen que se esta tratando pertenezca a una clase especifica. La clase resultante se calcula obteniendo el valor máximo. En nuestro ejemplo el clase 9."
      ]
    }
  ]
}