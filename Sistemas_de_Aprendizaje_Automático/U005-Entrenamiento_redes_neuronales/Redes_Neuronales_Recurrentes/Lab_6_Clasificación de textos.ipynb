{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ic4_occAAiAT"},"source":["#Comparativa clasificación de textos"]},{"cell_type":"markdown","metadata":{"id":"Eg62Pmz3o83v"},"source":["En esta parte, utilizaremos **embeddings** para resolver un problema de clasificación de texto. Los embeddings, representaciones distribuidas y vectoriales de elementos, son un concepto muy común en el mundo del deep learning. Los **word vectors** que hemos visto en clase son una representación en forma de embedding de las palabras.\n","\n","\n","Vamos a utilizar el dataset **\"Reuters newswire topics classification\"**, disponible desde Keras de manera similar al dataset de IMDB ([ver documentación](https://keras.io/datasets/#reuters-newswire-topics-classification)).\n","\n","---\n","\n","Tenemos varias opciones para entrenar modelos con embeddings. \n","\n","*   Utilizar una **Media de Embeddings** .\n","*  Utilizar una **RNN** sobre una secuencia de word vectors. Un buen consejo es emplear una red recurrente bidireccional.\n","*   Utilizar una **CNN** sobre una secuencia de word vectors. Aquí necesitamos cambiar un poco la idea de convolución para actuar sobre sequencias de vectores. Keras incluye una [Convolución en 1D](https://keras.io/layers/convolutional/#conv1d) que puede ser utilizada en este caso, con un ejemplo de uso en la documentación. Una forma de hacer funcionar este esquema sería utilizar la convolución en 1D + max pooling.\n","*  En esta práctica se pide implementar estos 3 modelos. Teniendo que ser el Acurracy de los datos Test por encima del **67 %**, en caso de no llegar en el primer modelo se deberá de realizar experimentos hasta llegar al desempeño objetivo.\n","---\n","\n","Dos **hiperparámetros** importantes a elegir en el modelo son la **longitud de las secuencias de texto** y el **tamaño del vocabulario** para los embeddings. Nótese que, al cortar todas las secuencias para que tengan el mismo tamaño, podríamos estar perdiendo mucho texto si elegimos un tamaño de secuencia demasiado pequeño. Igualmente, si las hacemos muy largas necesitaremos más tiempo para entrenar nuestros modelos. Una buena idea consiste en explorar los datos para ver cómo suelen ser de largos los textos y encontrar un buen trade-off para el tamaño de al secuencia.\n","\n","No utilizar Early Stopping y utilizar 50 epocas para los 3 tipos de modelos.\n","\n","---\n","\n","Los embeddings  se entrenan junto al modelo.  Una técnica frecuente es inicializar estos embeddings con word-vectors pre-entrenados en un gran corpus de texto. Esto puede ayudar ya que nuestro modelo empieza con unos embeddings que ya encapsulan significado. Si bien no es necesario para esta práctica, podéis ver cómo usar esta técnica [en el siguiente tutorial](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html).\n","\n","\n"]},{"cell_type":"markdown","source":["# Experimento 1: Media de Embeddings."],"metadata":{"id":"AzW2fRhkb1hK"}},{"cell_type":"markdown","source":["# Experimento 2: LSTM."],"metadata":{"id":"tC1Cx2NickTx"}},{"cell_type":"markdown","source":["# Experimento 3: CNN."],"metadata":{"id":"vlOEO6Zyg3-q"}}]}