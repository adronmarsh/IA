{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ic4_occAAiAT"},"source":["#Comparativa clasificación de textos"]},{"cell_type":"markdown","metadata":{"id":"Eg62Pmz3o83v"},"source":["En esta parte, utilizaremos **embeddings** para resolver un problema de clasificación de texto. Los embeddings, representaciones distribuidas y vectoriales de elementos, son un concepto muy común en el mundo del deep learning. Los **word vectors** que hemos visto en clase son una representación en forma de embedding de las palabras.\n","\n","\n","Vamos a utilizar el dataset **\"Reuters newswire topics classification\"**, disponible desde Keras de manera similar al dataset de IMDB ([ver documentación](https://keras.io/datasets/#reuters-newswire-topics-classification)).\n","\n","---\n","\n","Tenemos varias opciones para entrenar modelos con embeddings. \n","\n","*   Utilizar una **Media de Embeddings** .\n","*  Utilizar una **RNN** sobre una secuencia de word vectors. Un buen consejo es emplear una red recurrente bidireccional.\n","*   Utilizar una **CNN** sobre una secuencia de word vectors. Aquí necesitamos cambiar un poco la idea de convolución para actuar sobre sequencias de vectores. Keras incluye una [Convolución en 1D](https://keras.io/layers/convolutional/#conv1d) que puede ser utilizada en este caso, con un ejemplo de uso en la documentación. Una forma de hacer funcionar este esquema sería utilizar la convolución en 1D + max pooling.\n","*  En esta práctica se pide implementar estos 3 modelos. Teniendo que ser el Acurracy de los datos Test por encima del **67 %**, en caso de no llegar en el primer modelo se deberá de realizar experimentos hasta llegar al desempeño objetivo.\n","---\n","\n","Dos **hiperparámetros** importantes a elegir en el modelo son la **longitud de las secuencias de texto** y el **tamaño del vocabulario** para los embeddings. Nótese que, al cortar todas las secuencias para que tengan el mismo tamaño, podríamos estar perdiendo mucho texto si elegimos un tamaño de secuencia demasiado pequeño. Igualmente, si las hacemos muy largas necesitaremos más tiempo para entrenar nuestros modelos. Una buena idea consiste en explorar los datos para ver cómo suelen ser de largos los textos y encontrar un buen trade-off para el tamaño de al secuencia.\n","\n","No utilizar Early Stopping y utilizar 50 epocas para los 3 tipos de modelos.\n","\n","---\n","\n","Los embeddings  se entrenan junto al modelo.  Una técnica frecuente es inicializar estos embeddings con word-vectors pre-entrenados en un gran corpus de texto. Esto puede ayudar ya que nuestro modelo empieza con unos embeddings que ya encapsulan significado. Si bien no es necesario para esta práctica, podéis ver cómo usar esta técnica [en el siguiente tutorial](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html).\n","\n","\n"]},{"cell_type":"markdown","source":["# Experimento 1: Media de Embeddings."],"metadata":{"id":"AzW2fRhkb1hK"}},{"cell_type":"code","metadata":{"id":"2ew7HTbPpCJH"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","from keras.callbacks import EarlyStopping\n","from prettytable import PrettyTable\n","\n","\n","reuters = keras.datasets.reuters"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from numpy.matrixlib.defmatrix import matrix\n","#Con cuentas palabras voy a trabahar\n","VOCAB_SIZE =1000\n","SEQUENCE_LENGHT = 150\n","(train_data, train_labels),(test_data, test_labels) = reuters.load_data(num_words=VOCAB_SIZE)\n","print(\"Training entries: {}, Test entries: {}\".format(len(train_data),len(test_data)))\n","\n","#Tamaños de los textos para determinar longitud de las secuencias a usar\n","\n","lenghts = [len(x) for x in train_data]\n","print(\"Average length es {}\".format(np.mean(lenghts)))\n","print(\"Max length es {}\".format(max(lenghts)))\n","print(\"Median length es {}\".format(np.percentile(lenghts, 50)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rd0-WoBKl6FC","executionInfo":{"status":"ok","timestamp":1678469355783,"user_tz":-60,"elapsed":560,"user":{"displayName":"Sergio Aparicio","userId":"15567394745247109230"}},"outputId":"66f5fc6e-aa5e-4e93-8bc1-e34ad7275abd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training entries: 8982, Test entries: 2246\n","Average length es 145.5398574927633\n","Max length es 2376\n","Median length es 95.0\n"]}]},{"cell_type":"code","source":["#Nos hace la conversión. Cada índice esta asociado a un word vector, índice a partir del cual se leen los word embedding asociados\n","train_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1JhBIWMioeDx","executionInfo":{"status":"ok","timestamp":1678469355784,"user_tz":-60,"elapsed":8,"user":{"displayName":"Sergio Aparicio","userId":"15567394745247109230"}},"outputId":"93de3a5b-febf-4c86-ac2a-85f3e6877917"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 2, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 2, 2, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 2, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]),\n","       list([1, 2, 699, 2, 2, 56, 2, 2, 9, 56, 2, 2, 81, 5, 2, 57, 366, 737, 132, 20, 2, 7, 2, 49, 2, 2, 2, 2, 699, 2, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2, 2, 2, 775, 7, 48, 34, 191, 44, 35, 2, 505, 17, 12]),\n","       list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 2, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 2, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 2, 55, 2, 92, 617, 80, 2, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 2, 7, 2, 81, 5, 187, 11, 15, 9, 2, 201, 5, 47, 2, 18, 478, 2, 5, 2, 7, 232, 2, 71, 5, 160, 63, 11, 9, 2, 81, 5, 102, 59, 11, 17, 12]),\n","       ...,\n","       list([1, 141, 2, 387, 81, 8, 16, 2, 10, 340, 2, 850, 31, 56, 2, 691, 9, 2, 71, 9, 2, 2, 2, 699, 2, 2, 2, 699, 244, 2, 4, 49, 8, 4, 656, 850, 33, 2, 9, 2, 340, 2, 2, 9, 2, 22, 2, 2, 687, 83, 35, 15, 257, 6, 57, 2, 7, 4, 2, 654, 5, 2, 2, 2, 4, 49, 8, 16, 369, 646, 6, 2, 7, 124, 407, 17, 12]),\n","       list([1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, 258, 2, 18, 14, 74, 134, 2, 18, 88, 2, 72, 11, 14, 2, 32, 11, 123, 383, 89, 39, 46, 235, 10, 864, 728, 5, 258, 44, 11, 15, 22, 753, 9, 42, 92, 131, 728, 5, 69, 312, 11, 15, 22, 222, 2, 2, 383, 48, 39, 74, 235, 10, 864, 276, 5, 61, 32, 11, 15, 21, 4, 211, 5, 126, 2, 42, 92, 131, 46, 19, 352, 11, 15, 22, 710, 220, 9, 42, 92, 131, 276, 5, 59, 61, 11, 15, 22, 10, 455, 7, 2, 137, 336, 2, 6, 2, 142, 971, 2, 43, 359, 5, 4, 326, 753, 364, 17, 12]),\n","       list([1, 227, 2, 91, 2, 125, 2, 21, 4, 2, 76, 7, 4, 757, 481, 2, 790, 2, 2, 9, 111, 149, 8, 7, 10, 76, 223, 51, 4, 417, 8, 2, 91, 2, 2, 340, 7, 194, 2, 6, 2, 21, 127, 2, 2, 2, 6, 2, 4, 329, 433, 7, 65, 87, 2, 10, 2, 2, 290, 9, 21, 567, 16, 2, 24, 4, 76, 209, 30, 2, 2, 2, 8, 4, 60, 8, 4, 966, 308, 40, 2, 129, 2, 295, 277, 2, 9, 24, 286, 2, 234, 222, 9, 4, 906, 2, 2, 114, 2, 2, 7, 4, 113, 17, 12])],\n","      dtype=object)"]},"metadata":{},"execution_count":118}]},{"cell_type":"code","source":["test_labels\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6kQ1eXsfVRzz","executionInfo":{"status":"ok","timestamp":1678469355785,"user_tz":-60,"elapsed":7,"user":{"displayName":"Sergio Aparicio","userId":"15567394745247109230"}},"outputId":"cf6a384e-c067-4a72-a4a6-dfb7469abcf6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 3, 10,  1, ...,  3,  3, 24])"]},"metadata":{},"execution_count":119}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfkHHJjN9ERV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678469356018,"user_tz":-60,"elapsed":238,"user":{"displayName":"Sergio Aparicio","userId":"15567394745247109230"}},"outputId":"1aac58e6-734b-4ce8-e339-6f79ead15e4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tenemos las siguientes clases 46\n"]}],"source":["word_index = reuters.get_word_index()\n","\n","#Los primeros índices son reservados. \n","#Asignamos un valor para inicio de cadena, padding. A nuestro word index original le estamos añadiendo estos tokens especiales\n","\n","word_index = {k:(v+3) for k,v in word_index.items()}\n","word_index[\"<PAD>\"] = 0 \n","word_index[\"<START>\"] = 1 \n","word_index[\"<UNK>\"] = 2 #No conocido\n","word_index[\"<UNUSED>\"] = 3\n","\n","#Si sabemos un índice podemos saber la palabra. \n","\n","reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n","\n","def decode_review(text):\n","    return ''.join([reverse_word_index.get(i, '?') for i in text])\n","\n","\n","X_train = keras.preprocessing. sequence.pad_sequences(train_data, \n","                                                        value=word_index[\"<PAD>\"], \n","                                                        padding='post', \n","                                                        maxlen=SEQUENCE_LENGHT)\n","\n","X_test = keras.preprocessing. sequence.pad_sequences(test_data, \n","                                                        value=word_index[\"<PAD>\"], \n","                                                        padding='post', \n","                                                        maxlen=SEQUENCE_LENGHT)\n","\n","y_train= keras.utils.to_categorical(train_labels)\n","y_test= keras.utils.to_categorical(test_labels)\n","\n","num_clases= y_train.shape[1]\n","\n","print(\"Tenemos las siguientes clases {}\".format(num_clases))\n"]},{"cell_type":"code","source":["vocab_size = 10000\n","\n","model = keras.Sequential()\n"," \n","\n","model.add(keras.layers.Embedding(vocab_size,64)) \n","#Los promedia los va a aplanar.\n","model.add(keras.layers.GlobalAveragePooling1D())\n","#Ahora si, una capa densa con relu\n","model.add(keras.layers.Dense(128, activation=tf.nn.relu)) # Nuestra ya conocida capa densa\n","\n","model.add(keras.layers.Dense(num_clases, activation=tf.nn.softmax))\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51PefRcJYOMa","executionInfo":{"status":"ok","timestamp":1678469356018,"user_tz":-60,"elapsed":6,"user":{"displayName":"Sergio Aparicio","userId":"15567394745247109230"}},"outputId":"996c7fa5-91e7-405f-a716-b3846f4f16fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_29\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_29 (Embedding)    (None, None, 64)          640000    \n","                                                                 \n"," global_average_pooling1d_15  (None, 64)               0         \n","  (GlobalAveragePooling1D)                                       \n","                                                                 \n"," dense_52 (Dense)            (None, 128)               8320      \n","                                                                 \n"," dense_53 (Dense)            (None, 46)                5934      \n","                                                                 \n","=================================================================\n","Total params: 654,254\n","Trainable params: 654,254\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.compile(optimizer='adam', \n","             loss='binary_crossentropy', \n","             metrics=['accuracy'])\n","\n","\n","history = model.fit(X_train,\n","                    y_train,\n","                    epochs=50, \n","                    batch_size=512, \n","                    verbose=1,\n","                    validation_split=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KhDqjXULYaS6","executionInfo":{"status":"ok","timestamp":1678469365703,"user_tz":-60,"elapsed":9687,"user":{"displayName":"Sergio Aparicio","userId":"15567394745247109230"}},"outputId":"fc0b765a-04b4-43ee-8cc8-1693bde69548"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","16/16 [==============================] - 3s 65ms/step - loss: 0.1033 - accuracy: 0.3177 - val_loss: 0.1013 - val_accuracy: 0.3315\n","Epoch 2/50\n","16/16 [==============================] - 0s 15ms/step - loss: 0.0980 - accuracy: 0.3540 - val_loss: 0.0929 - val_accuracy: 0.3315\n","Epoch 3/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0853 - accuracy: 0.3540 - val_loss: 0.0764 - val_accuracy: 0.3315\n","Epoch 4/50\n","16/16 [==============================] - 0s 14ms/step - loss: 0.0713 - accuracy: 0.3540 - val_loss: 0.0710 - val_accuracy: 0.3315\n","Epoch 5/50\n","16/16 [==============================] - 0s 15ms/step - loss: 0.0686 - accuracy: 0.3540 - val_loss: 0.0690 - val_accuracy: 0.3315\n","Epoch 6/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0671 - accuracy: 0.3540 - val_loss: 0.0680 - val_accuracy: 0.3315\n","Epoch 7/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0658 - accuracy: 0.3639 - val_loss: 0.0668 - val_accuracy: 0.3737\n","Epoch 8/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0644 - accuracy: 0.3867 - val_loss: 0.0653 - val_accuracy: 0.3760\n","Epoch 9/50\n","16/16 [==============================] - 0s 15ms/step - loss: 0.0628 - accuracy: 0.3996 - val_loss: 0.0635 - val_accuracy: 0.3993\n","Epoch 10/50\n","16/16 [==============================] - 0s 15ms/step - loss: 0.0610 - accuracy: 0.4239 - val_loss: 0.0617 - val_accuracy: 0.4260\n","Epoch 11/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0594 - accuracy: 0.4502 - val_loss: 0.0600 - val_accuracy: 0.4360\n","Epoch 12/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0579 - accuracy: 0.4685 - val_loss: 0.0586 - val_accuracy: 0.4716\n","Epoch 13/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0565 - accuracy: 0.4886 - val_loss: 0.0571 - val_accuracy: 0.4861\n","Epoch 14/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0552 - accuracy: 0.5062 - val_loss: 0.0558 - val_accuracy: 0.5083\n","Epoch 15/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0538 - accuracy: 0.5263 - val_loss: 0.0545 - val_accuracy: 0.5273\n","Epoch 16/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0525 - accuracy: 0.5399 - val_loss: 0.0531 - val_accuracy: 0.5362\n","Epoch 17/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0512 - accuracy: 0.5498 - val_loss: 0.0519 - val_accuracy: 0.5484\n","Epoch 18/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0500 - accuracy: 0.5628 - val_loss: 0.0509 - val_accuracy: 0.5695\n","Epoch 19/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0489 - accuracy: 0.5752 - val_loss: 0.0501 - val_accuracy: 0.5818\n","Epoch 20/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0480 - accuracy: 0.5867 - val_loss: 0.0494 - val_accuracy: 0.5895\n","Epoch 21/50\n","16/16 [==============================] - 0s 15ms/step - loss: 0.0472 - accuracy: 0.5988 - val_loss: 0.0487 - val_accuracy: 0.5940\n","Epoch 22/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0465 - accuracy: 0.6084 - val_loss: 0.0482 - val_accuracy: 0.6040\n","Epoch 23/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0458 - accuracy: 0.6150 - val_loss: 0.0476 - val_accuracy: 0.6174\n","Epoch 24/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0451 - accuracy: 0.6251 - val_loss: 0.0470 - val_accuracy: 0.6240\n","Epoch 25/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0444 - accuracy: 0.6352 - val_loss: 0.0464 - val_accuracy: 0.6285\n","Epoch 26/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0436 - accuracy: 0.6469 - val_loss: 0.0456 - val_accuracy: 0.6396\n","Epoch 27/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0427 - accuracy: 0.6548 - val_loss: 0.0448 - val_accuracy: 0.6474\n","Epoch 28/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0418 - accuracy: 0.6618 - val_loss: 0.0440 - val_accuracy: 0.6596\n","Epoch 29/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0409 - accuracy: 0.6697 - val_loss: 0.0433 - val_accuracy: 0.6641\n","Epoch 30/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0401 - accuracy: 0.6757 - val_loss: 0.0426 - val_accuracy: 0.6685\n","Epoch 31/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0393 - accuracy: 0.6840 - val_loss: 0.0420 - val_accuracy: 0.6752\n","Epoch 32/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0385 - accuracy: 0.6893 - val_loss: 0.0414 - val_accuracy: 0.6774\n","Epoch 33/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0378 - accuracy: 0.6953 - val_loss: 0.0409 - val_accuracy: 0.6752\n","Epoch 34/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0372 - accuracy: 0.6992 - val_loss: 0.0405 - val_accuracy: 0.6796\n","Epoch 35/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0366 - accuracy: 0.7060 - val_loss: 0.0402 - val_accuracy: 0.6863\n","Epoch 36/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0361 - accuracy: 0.7094 - val_loss: 0.0398 - val_accuracy: 0.6874\n","Epoch 37/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0356 - accuracy: 0.7129 - val_loss: 0.0395 - val_accuracy: 0.6919\n","Epoch 38/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0351 - accuracy: 0.7153 - val_loss: 0.0392 - val_accuracy: 0.6908\n","Epoch 39/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0347 - accuracy: 0.7176 - val_loss: 0.0390 - val_accuracy: 0.6997\n","Epoch 40/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0343 - accuracy: 0.7220 - val_loss: 0.0388 - val_accuracy: 0.6941\n","Epoch 41/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0339 - accuracy: 0.7226 - val_loss: 0.0386 - val_accuracy: 0.6974\n","Epoch 42/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0335 - accuracy: 0.7267 - val_loss: 0.0385 - val_accuracy: 0.6997\n","Epoch 43/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0331 - accuracy: 0.7279 - val_loss: 0.0383 - val_accuracy: 0.6974\n","Epoch 44/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0328 - accuracy: 0.7302 - val_loss: 0.0381 - val_accuracy: 0.7019\n","Epoch 45/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0324 - accuracy: 0.7348 - val_loss: 0.0380 - val_accuracy: 0.7008\n","Epoch 46/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0321 - accuracy: 0.7406 - val_loss: 0.0380 - val_accuracy: 0.6997\n","Epoch 47/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0318 - accuracy: 0.7435 - val_loss: 0.0377 - val_accuracy: 0.7075\n","Epoch 48/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0314 - accuracy: 0.7498 - val_loss: 0.0377 - val_accuracy: 0.7108\n","Epoch 49/50\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0311 - accuracy: 0.7512 - val_loss: 0.0376 - val_accuracy: 0.7108\n","Epoch 50/50\n","16/16 [==============================] - 0s 7ms/step - loss: 0.0308 - accuracy: 0.7532 - val_loss: 0.0376 - val_accuracy: 0.7119\n"]}]},{"cell_type":"code","source":["results = model.evaluate(X_test, y_test)\n","print(results)\n","acurracyExp0=results[1]\n","myTable = PrettyTable([\"Experimento\", \" Test Accuracy\"])\n","myTable.add_row([\"Media Embedings\", \"{0:.5f}\".format(acurracyExp0)])\n","print(myTable)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ND43ylhbcBi","executionInfo":{"status":"ok","timestamp":1678469365959,"user_tz":-60,"elapsed":264,"user":{"displayName":"Sergio Aparicio","userId":"15567394745247109230"}},"outputId":"bba2ccac-4242-4354-976b-9309e5d0b82a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["71/71 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.7084\n","[0.03733041137456894, 0.7083704471588135]\n","+-----------------+----------------+\n","|   Experimento   |  Test Accuracy |\n","+-----------------+----------------+\n","| Media Embedings |    0.70837     |\n","+-----------------+----------------+\n"]}]},{"cell_type":"markdown","source":["# Experimento 2: LSTM."],"metadata":{"id":"tC1Cx2NickTx"}},{"cell_type":"code","source":["model = keras.Sequential()\n"," \n","\n","model.add(keras.layers.Embedding(vocab_size,64)) \n","model.add(keras.layers.Bidirectional(keras.layers.LSTM(64)))\n","#Ahora si, una capa densa con relu\n","model.add(keras.layers.Dense(128, activation=tf.nn.relu)) # Nuestra ya conocida capa densa\n","\n","model.add(keras.layers.Dense(num_clases, activation=tf.nn.softmax))\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fFKQEzD9drlQ","executionInfo":{"status":"ok","timestamp":1678469366326,"user_tz":-60,"elapsed":369,"user":{"displayName":"Sergio Aparicio","userId":"15567394745247109230"}},"outputId":"65d61c69-815d-4f28-c71b-e8ff3b93f151"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_30\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_30 (Embedding)    (None, None, 64)          640000    \n","                                                                 \n"," bidirectional_6 (Bidirectio  (None, 128)              66048     \n"," nal)                                                            \n","                                                                 \n"," dense_54 (Dense)            (None, 128)               16512     \n","                                                                 \n"," dense_55 (Dense)            (None, 46)                5934      \n","                                                                 \n","=================================================================\n","Total params: 728,494\n","Trainable params: 728,494\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.compile(optimizer='adam', \n","             loss='binary_crossentropy', \n","             metrics=['accuracy'])\n","\n","\n","history = model.fit(X_train,\n","                    y_train,\n","                    epochs=50, \n","                    batch_size=512, \n","                    verbose=1,\n","                    validation_split=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xee7OkYoePtA","executionInfo":{"status":"ok","timestamp":1678469395746,"user_tz":-60,"elapsed":29425,"user":{"displayName":"Sergio Aparicio","userId":"15567394745247109230"}},"outputId":"e1d9fbae-ebda-404b-d62c-135f2de79cad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","16/16 [==============================] - 6s 116ms/step - loss: 0.1006 - accuracy: 0.3095 - val_loss: 0.0845 - val_accuracy: 0.3315\n","Epoch 2/50\n","16/16 [==============================] - 1s 35ms/step - loss: 0.0747 - accuracy: 0.3267 - val_loss: 0.0707 - val_accuracy: 0.3537\n","Epoch 3/50\n","16/16 [==============================] - 1s 34ms/step - loss: 0.0653 - accuracy: 0.3896 - val_loss: 0.0609 - val_accuracy: 0.4883\n","Epoch 4/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0584 - accuracy: 0.5062 - val_loss: 0.0584 - val_accuracy: 0.4917\n","Epoch 5/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0549 - accuracy: 0.5249 - val_loss: 0.0531 - val_accuracy: 0.5562\n","Epoch 6/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0505 - accuracy: 0.5644 - val_loss: 0.0510 - val_accuracy: 0.5662\n","Epoch 7/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0486 - accuracy: 0.5763 - val_loss: 0.0499 - val_accuracy: 0.5873\n","Epoch 8/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0473 - accuracy: 0.5816 - val_loss: 0.0490 - val_accuracy: 0.5784\n","Epoch 9/50\n","16/16 [==============================] - 1s 35ms/step - loss: 0.0464 - accuracy: 0.5890 - val_loss: 0.0485 - val_accuracy: 0.5829\n","Epoch 10/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0457 - accuracy: 0.5963 - val_loss: 0.0482 - val_accuracy: 0.5873\n","Epoch 11/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0452 - accuracy: 0.6081 - val_loss: 0.0481 - val_accuracy: 0.5884\n","Epoch 12/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0445 - accuracy: 0.6196 - val_loss: 0.0472 - val_accuracy: 0.6007\n","Epoch 13/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0440 - accuracy: 0.6208 - val_loss: 0.0470 - val_accuracy: 0.6018\n","Epoch 14/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0430 - accuracy: 0.6378 - val_loss: 0.0463 - val_accuracy: 0.6062\n","Epoch 15/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0417 - accuracy: 0.6556 - val_loss: 0.0460 - val_accuracy: 0.6185\n","Epoch 16/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0407 - accuracy: 0.6637 - val_loss: 0.0454 - val_accuracy: 0.6251\n","Epoch 17/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0398 - accuracy: 0.6677 - val_loss: 0.0449 - val_accuracy: 0.6340\n","Epoch 18/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0388 - accuracy: 0.6813 - val_loss: 0.0439 - val_accuracy: 0.6418\n","Epoch 19/50\n","16/16 [==============================] - 0s 31ms/step - loss: 0.0385 - accuracy: 0.6825 - val_loss: 0.0466 - val_accuracy: 0.6162\n","Epoch 20/50\n","16/16 [==============================] - 0s 31ms/step - loss: 0.0390 - accuracy: 0.6720 - val_loss: 0.0454 - val_accuracy: 0.6251\n","Epoch 21/50\n","16/16 [==============================] - 0s 30ms/step - loss: 0.0372 - accuracy: 0.6921 - val_loss: 0.0435 - val_accuracy: 0.6507\n","Epoch 22/50\n","16/16 [==============================] - 0s 31ms/step - loss: 0.0358 - accuracy: 0.7043 - val_loss: 0.0431 - val_accuracy: 0.6618\n","Epoch 23/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0350 - accuracy: 0.7133 - val_loss: 0.0433 - val_accuracy: 0.6685\n","Epoch 24/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0342 - accuracy: 0.7176 - val_loss: 0.0427 - val_accuracy: 0.6719\n","Epoch 25/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0336 - accuracy: 0.7266 - val_loss: 0.0421 - val_accuracy: 0.6696\n","Epoch 26/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0330 - accuracy: 0.7308 - val_loss: 0.0422 - val_accuracy: 0.6685\n","Epoch 27/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0322 - accuracy: 0.7408 - val_loss: 0.0425 - val_accuracy: 0.6752\n","Epoch 28/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0318 - accuracy: 0.7442 - val_loss: 0.0425 - val_accuracy: 0.6707\n","Epoch 29/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0312 - accuracy: 0.7471 - val_loss: 0.0426 - val_accuracy: 0.6707\n","Epoch 30/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0304 - accuracy: 0.7544 - val_loss: 0.0427 - val_accuracy: 0.6607\n","Epoch 31/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0300 - accuracy: 0.7560 - val_loss: 0.0427 - val_accuracy: 0.6763\n","Epoch 32/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0299 - accuracy: 0.7585 - val_loss: 0.0424 - val_accuracy: 0.6741\n","Epoch 33/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0291 - accuracy: 0.7647 - val_loss: 0.0429 - val_accuracy: 0.6652\n","Epoch 34/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0287 - accuracy: 0.7661 - val_loss: 0.0428 - val_accuracy: 0.6719\n","Epoch 35/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0279 - accuracy: 0.7742 - val_loss: 0.0442 - val_accuracy: 0.6774\n","Epoch 36/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0277 - accuracy: 0.7757 - val_loss: 0.0448 - val_accuracy: 0.6719\n","Epoch 37/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0272 - accuracy: 0.7815 - val_loss: 0.0443 - val_accuracy: 0.6796\n","Epoch 38/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0274 - accuracy: 0.7789 - val_loss: 0.0445 - val_accuracy: 0.6852\n","Epoch 39/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0268 - accuracy: 0.7846 - val_loss: 0.0437 - val_accuracy: 0.6885\n","Epoch 40/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0265 - accuracy: 0.7873 - val_loss: 0.0431 - val_accuracy: 0.6885\n","Epoch 41/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0259 - accuracy: 0.7945 - val_loss: 0.0445 - val_accuracy: 0.6952\n","Epoch 42/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0256 - accuracy: 0.7969 - val_loss: 0.0448 - val_accuracy: 0.6897\n","Epoch 43/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0254 - accuracy: 0.7974 - val_loss: 0.0449 - val_accuracy: 0.6696\n","Epoch 44/50\n","16/16 [==============================] - 0s 28ms/step - loss: 0.0255 - accuracy: 0.7972 - val_loss: 0.0450 - val_accuracy: 0.6852\n","Epoch 45/50\n","16/16 [==============================] - 0s 30ms/step - loss: 0.0250 - accuracy: 0.8035 - val_loss: 0.0443 - val_accuracy: 0.6852\n","Epoch 46/50\n","16/16 [==============================] - 0s 31ms/step - loss: 0.0243 - accuracy: 0.8082 - val_loss: 0.0447 - val_accuracy: 0.6919\n","Epoch 47/50\n","16/16 [==============================] - 0s 31ms/step - loss: 0.0241 - accuracy: 0.8096 - val_loss: 0.0447 - val_accuracy: 0.6897\n","Epoch 48/50\n","16/16 [==============================] - 1s 33ms/step - loss: 0.0237 - accuracy: 0.8128 - val_loss: 0.0452 - val_accuracy: 0.6919\n","Epoch 49/50\n","16/16 [==============================] - 1s 33ms/step - loss: 0.0231 - accuracy: 0.8181 - val_loss: 0.0459 - val_accuracy: 0.7008\n","Epoch 50/50\n","16/16 [==============================] - 0s 29ms/step - loss: 0.0228 - accuracy: 0.8193 - val_loss: 0.0455 - val_accuracy: 0.6997\n"]}]},{"cell_type":"code","source":["results = model.evaluate(X_test, y_test)\n","print(results)\n","acurracyExp1=results[1]\n","myTable.add_row([\"LSTM\", \"{0:.5f}\".format(acurracyExp1)])\n","print(myTable)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9SQEp777e7cR","executionInfo":{"status":"ok","timestamp":1678469396309,"user_tz":-60,"elapsed":567,"user":{"displayName":"Sergio Aparicio","userId":"15567394745247109230"}},"outputId":"7b0c2cec-ec4a-4e6a-ee71-98d40b16c188"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["71/71 [==============================] - 0s 6ms/step - loss: 0.0443 - accuracy: 0.6888\n","[0.04430641606450081, 0.6887800693511963]\n","+-----------------+----------------+\n","|   Experimento   |  Test Accuracy |\n","+-----------------+----------------+\n","| Media Embedings |    0.70837     |\n","|       LSTM      |    0.68878     |\n","+-----------------+----------------+\n"]}]},{"cell_type":"markdown","source":["# Experimento 3: CNN."],"metadata":{"id":"vlOEO6Zyg3-q"}},{"cell_type":"code","source":["from keras.api._v2.keras import activations\n","vocab_size = 10000\n","\n","model = keras.Sequential()\n"," \n","\n","model.add(keras.layers.Embedding(vocab_size,64)) \n","model.add(keras.layers.Dropout(0.5))\n","model.add(keras.layers.Conv1D(filters=16,kernel_size=3,activation='relu'))\n","model.add(keras.layers.GlobalAveragePooling1D())\n","\n","#Ahora si, una capa densa con relu\n","model.add(keras.layers.Dense(128, activation=tf.nn.relu)) # Nuestra ya conocida capa densa\n","#Ahora si, una capa densa con relu\n","model.add(keras.layers.Dense(128, activation=tf.nn.relu)) # Nuestra ya conocida capa densa\n","\n","#Una última capa que tiene 1 elemento. La sigmoid se identifica más con una probabilidad.\n","model.add(keras.layers.Dense(num_clases, activation=tf.nn.softmax))\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZ0vCh5JhAdv","executionInfo":{"status":"ok","timestamp":1678469397314,"user_tz":-60,"elapsed":1014,"user":{"displayName":"Sergio Aparicio","userId":"15567394745247109230"}},"outputId":"4727ae3d-755e-4577-ae6b-b63b7ba60459"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_31\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_31 (Embedding)    (None, None, 64)          640000    \n","                                                                 \n"," dropout_15 (Dropout)        (None, None, 64)          0         \n","                                                                 \n"," conv1d_11 (Conv1D)          (None, None, 16)          3088      \n","                                                                 \n"," global_average_pooling1d_16  (None, 16)               0         \n","  (GlobalAveragePooling1D)                                       \n","                                                                 \n"," dense_56 (Dense)            (None, 128)               2176      \n","                                                                 \n"," dense_57 (Dense)            (None, 128)               16512     \n","                                                                 \n"," dense_58 (Dense)            (None, 46)                5934      \n","                                                                 \n","=================================================================\n","Total params: 667,710\n","Trainable params: 667,710\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.compile(optimizer='adam', \n","             loss='binary_crossentropy', \n","             metrics=['accuracy'])\n","\n","\n","history = model.fit(X_train,\n","                    y_train,\n","                    epochs=50, \n","                    batch_size=512, \n","                    verbose=1,\n","                    validation_split=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678469408236,"user_tz":-60,"elapsed":10931,"user":{"displayName":"Sergio Aparicio","userId":"15567394745247109230"}},"outputId":"9cce3e4f-c459-4701-9150-1984a35b8d01","id":"haEjJctJh5Ty"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","16/16 [==============================] - 3s 51ms/step - loss: 0.1032 - accuracy: 0.3390 - val_loss: 0.1004 - val_accuracy: 0.3315\n","Epoch 2/50\n","16/16 [==============================] - 0s 19ms/step - loss: 0.0919 - accuracy: 0.3540 - val_loss: 0.0770 - val_accuracy: 0.3315\n","Epoch 3/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0714 - accuracy: 0.3540 - val_loss: 0.0698 - val_accuracy: 0.3315\n","Epoch 4/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0677 - accuracy: 0.3540 - val_loss: 0.0683 - val_accuracy: 0.3315\n","Epoch 5/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0660 - accuracy: 0.3604 - val_loss: 0.0665 - val_accuracy: 0.3682\n","Epoch 6/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0636 - accuracy: 0.3990 - val_loss: 0.0632 - val_accuracy: 0.4138\n","Epoch 7/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0601 - accuracy: 0.4626 - val_loss: 0.0595 - val_accuracy: 0.4761\n","Epoch 8/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0571 - accuracy: 0.4996 - val_loss: 0.0567 - val_accuracy: 0.5039\n","Epoch 9/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0545 - accuracy: 0.5222 - val_loss: 0.0541 - val_accuracy: 0.5228\n","Epoch 10/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0518 - accuracy: 0.5439 - val_loss: 0.0519 - val_accuracy: 0.5395\n","Epoch 11/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0497 - accuracy: 0.5619 - val_loss: 0.0504 - val_accuracy: 0.5662\n","Epoch 12/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0484 - accuracy: 0.5739 - val_loss: 0.0496 - val_accuracy: 0.5784\n","Epoch 13/50\n","16/16 [==============================] - 0s 17ms/step - loss: 0.0475 - accuracy: 0.5813 - val_loss: 0.0490 - val_accuracy: 0.5829\n","Epoch 14/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0467 - accuracy: 0.5874 - val_loss: 0.0485 - val_accuracy: 0.5873\n","Epoch 15/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0459 - accuracy: 0.6009 - val_loss: 0.0484 - val_accuracy: 0.5962\n","Epoch 16/50\n","16/16 [==============================] - 0s 17ms/step - loss: 0.0454 - accuracy: 0.6045 - val_loss: 0.0477 - val_accuracy: 0.5962\n","Epoch 17/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0445 - accuracy: 0.6100 - val_loss: 0.0472 - val_accuracy: 0.6062\n","Epoch 18/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0437 - accuracy: 0.6161 - val_loss: 0.0467 - val_accuracy: 0.6129\n","Epoch 19/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0431 - accuracy: 0.6248 - val_loss: 0.0462 - val_accuracy: 0.6118\n","Epoch 20/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0424 - accuracy: 0.6297 - val_loss: 0.0462 - val_accuracy: 0.6196\n","Epoch 21/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0417 - accuracy: 0.6390 - val_loss: 0.0458 - val_accuracy: 0.6263\n","Epoch 22/50\n","16/16 [==============================] - 0s 16ms/step - loss: 0.0412 - accuracy: 0.6453 - val_loss: 0.0452 - val_accuracy: 0.6274\n","Epoch 23/50\n","16/16 [==============================] - 0s 17ms/step - loss: 0.0406 - accuracy: 0.6506 - val_loss: 0.0450 - val_accuracy: 0.6274\n","Epoch 24/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0401 - accuracy: 0.6559 - val_loss: 0.0447 - val_accuracy: 0.6352\n","Epoch 25/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0395 - accuracy: 0.6670 - val_loss: 0.0445 - val_accuracy: 0.6418\n","Epoch 26/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0389 - accuracy: 0.6759 - val_loss: 0.0444 - val_accuracy: 0.6429\n","Epoch 27/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0383 - accuracy: 0.6808 - val_loss: 0.0439 - val_accuracy: 0.6507\n","Epoch 28/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0378 - accuracy: 0.6898 - val_loss: 0.0434 - val_accuracy: 0.6585\n","Epoch 29/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0372 - accuracy: 0.6934 - val_loss: 0.0431 - val_accuracy: 0.6563\n","Epoch 30/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0367 - accuracy: 0.6990 - val_loss: 0.0428 - val_accuracy: 0.6630\n","Epoch 31/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0362 - accuracy: 0.7027 - val_loss: 0.0428 - val_accuracy: 0.6696\n","Epoch 32/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0357 - accuracy: 0.7048 - val_loss: 0.0423 - val_accuracy: 0.6774\n","Epoch 33/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0353 - accuracy: 0.7091 - val_loss: 0.0423 - val_accuracy: 0.6741\n","Epoch 34/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0351 - accuracy: 0.7086 - val_loss: 0.0423 - val_accuracy: 0.6663\n","Epoch 35/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0346 - accuracy: 0.7126 - val_loss: 0.0421 - val_accuracy: 0.6774\n","Epoch 36/50\n","16/16 [==============================] - 0s 14ms/step - loss: 0.0341 - accuracy: 0.7156 - val_loss: 0.0413 - val_accuracy: 0.6796\n","Epoch 37/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0338 - accuracy: 0.7172 - val_loss: 0.0416 - val_accuracy: 0.6730\n","Epoch 38/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0334 - accuracy: 0.7203 - val_loss: 0.0410 - val_accuracy: 0.6852\n","Epoch 39/50\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0332 - accuracy: 0.7246 - val_loss: 0.0410 - val_accuracy: 0.6841\n","Epoch 40/50\n","16/16 [==============================] - 0s 13ms/step - loss: 0.0329 - accuracy: 0.7240 - val_loss: 0.0409 - val_accuracy: 0.6874\n","Epoch 41/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0326 - accuracy: 0.7270 - val_loss: 0.0409 - val_accuracy: 0.6796\n","Epoch 42/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0323 - accuracy: 0.7301 - val_loss: 0.0402 - val_accuracy: 0.6852\n","Epoch 43/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0320 - accuracy: 0.7315 - val_loss: 0.0403 - val_accuracy: 0.6841\n","Epoch 44/50\n","16/16 [==============================] - 0s 13ms/step - loss: 0.0317 - accuracy: 0.7330 - val_loss: 0.0407 - val_accuracy: 0.6863\n","Epoch 45/50\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0316 - accuracy: 0.7360 - val_loss: 0.0402 - val_accuracy: 0.6863\n","Epoch 46/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0312 - accuracy: 0.7425 - val_loss: 0.0403 - val_accuracy: 0.6908\n","Epoch 47/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0310 - accuracy: 0.7435 - val_loss: 0.0405 - val_accuracy: 0.6897\n","Epoch 48/50\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0307 - accuracy: 0.7477 - val_loss: 0.0404 - val_accuracy: 0.6941\n","Epoch 49/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0306 - accuracy: 0.7490 - val_loss: 0.0404 - val_accuracy: 0.6930\n","Epoch 50/50\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0302 - accuracy: 0.7527 - val_loss: 0.0403 - val_accuracy: 0.6930\n"]}]},{"cell_type":"code","source":["results = model.evaluate(X_test, y_test)\n","print(results)\n","acurracyExp1=results[1]\n","myTable.add_row([\"CNN 1D\", \"{0:.5f}\".format(acurracyExp1)])\n","print(myTable)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPyX6Hy1iAvm","executionInfo":{"status":"ok","timestamp":1678469408647,"user_tz":-60,"elapsed":420,"user":{"displayName":"Sergio Aparicio","userId":"15567394745247109230"}},"outputId":"1b8bc9b4-e0f6-42d0-d4eb-17c6783be1a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["71/71 [==============================] - 0s 3ms/step - loss: 0.0401 - accuracy: 0.6790\n","[0.040084440261125565, 0.6789848804473877]\n","+-----------------+----------------+\n","|   Experimento   |  Test Accuracy |\n","+-----------------+----------------+\n","| Media Embedings |    0.70837     |\n","|       LSTM      |    0.68878     |\n","|      CNN 1D     |    0.67898     |\n","+-----------------+----------------+\n"]}]}]}