{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def obtener_texto_wikipedia(url):\n",
    "    # Hacer una solicitud HTTP para obtener el contenido de la página\n",
    "    respuesta = requests.get(url)\n",
    "\n",
    "    # Parsear el contenido HTML de la página\n",
    "    soup = BeautifulSoup(respuesta.text, 'html.parser')\n",
    "\n",
    "    # Extraer el texto del contenido principal de la página de Wikipedia\n",
    "    contenido = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    texto = contenido.get_text(separator='\\n')\n",
    "\n",
    "    return texto\n",
    "\n",
    "# URL de la página de Wikipedia sobre Inteligencia Artificial\n",
    "url = 'https://es.wikipedia.org/wiki/Inteligencia_artificial'\n",
    "\n",
    "# Obtener el texto\n",
    "texto_wikipedia = obtener_texto_wikipedia(url)\n",
    "\n",
    "# Imprimir los primeros 500 caracteres para verificar\n",
    "# print(texto_wikipedia[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acuarela alan turing generada mediante inteligencia artificial considerado padre mismacitarequeridala inteligencia artificial ia contexto ciencias computación disciplina conjunto capacidades cognoscitivas intelectuales expresadas sistemas informáticos combinaciones algoritmos cuyo propósito creación máquinas imiten inteligencia humana realizar tareas pueden mejorar conforme recopilen información hizo presente después segunda guerra mundial desarrollo prueba turing mientras locución acuñada infor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amartinezgil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amartinezgil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    # Eliminar etiquetas HTML\n",
    "    texto = re.sub(r'<[^>]+>', '', texto)\n",
    "\n",
    "    # Eliminar referencias y enlaces\n",
    "    texto = re.sub(r'\\[\\d+\\]', '', texto)\n",
    "\n",
    "    # Convertir a minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Eliminar puntuación y números\n",
    "    texto = re.sub(r'[^a-záéíóúñ ]', '', texto)\n",
    "\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(texto, language='spanish')\n",
    "\n",
    "    # Eliminar palabras de parada\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [palabra for palabra in tokens if palabra not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Aplicar la función de limpieza al texto\n",
    "texto_limpio = limpiar_texto(texto_wikipedia)\n",
    "\n",
    "print(texto_limpio[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar preguntas a partir de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def generar_preguntas_respuestas(texto):\n",
    "    preguntas_respuestas = []\n",
    "    oraciones = sent_tokenize(texto)\n",
    "\n",
    "    for oracion in oraciones:\n",
    "        # Aquí implementas una lógica para formular una pregunta basada en la oración\n",
    "        pregunta = \"Que es la IA?\"\n",
    "        respuesta = oracion  # En este caso simple, la oración completa es la respuesta\n",
    "\n",
    "        preguntas_respuestas.append({\n",
    "            \"context\": texto,\n",
    "            \"question\": pregunta,\n",
    "            \"answer\": respuesta\n",
    "        })\n",
    "\n",
    "    return preguntas_respuestas\n",
    "\n",
    "# Ejemplo de uso\n",
    "conjunto_datos = generar_preguntas_respuestas(texto_limpio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir los datos en formato BERT y pasarlo a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def formatear_para_bert(preguntas_respuestas):\n",
    "    formato_bert = []\n",
    "\n",
    "    for pr in preguntas_respuestas:\n",
    "        entrada = {\n",
    "            'context': pr['context'],\n",
    "            'qas': [{\n",
    "                'question': pr['question'],\n",
    "                'answers': [{\n",
    "                    'text': pr['answer'],\n",
    "                    'answer_start': pr['context'].find(pr['answer'])\n",
    "                }]\n",
    "            }]\n",
    "        }\n",
    "        formato_bert.append(entrada)\n",
    "\n",
    "    return formato_bert\n",
    "\n",
    "# Convertir a formato BERT\n",
    "datos_bert = formatear_para_bert(conjunto_datos)\n",
    "\n",
    "# Guardar en un archivo JSON\n",
    "with open('datos_entrenamiento_bert.json', 'w') as f:\n",
    "    json.dump(datos_bert, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amartinezgil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\amartinezgil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16370 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "with open('datos_entrenamiento_bert.json', 'r') as f:\n",
    "    conjunto_datos = json.load(f)\n",
    "\n",
    "# Cargar el tokenizer y el modelo BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preparar los datos para BERT\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for item in conjunto_datos:\n",
    "    contexto = item['context']\n",
    "    \n",
    "    for qa in item['qas']:\n",
    "        pregunta = qa['question']\n",
    "        respuesta = qa['answers'][0]['text']\n",
    "        respuesta_start = qa['answers'][0]['answer_start']\n",
    "\n",
    "        # Codificar contexto y pregunta\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            contexto,\n",
    "                            pregunta,\n",
    "                            max_length=512,\n",
    "                            pad_to_max_length=True,\n",
    "                            return_attention_mask=True,\n",
    "                            truncation=True,\n",
    "                            return_tensors='pt'\n",
    "                       )\n",
    "        \n",
    "        # Agregar input_ids y attention_masks\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # Convertir respuesta_start a token index\n",
    "        respuesta_span = tokenizer.encode(respuesta, add_special_tokens=False)\n",
    "        respuesta_start_index = encoded_dict['input_ids'][0].tolist().index(respuesta_span[0])\n",
    "        respuesta_end_index = respuesta_start_index + len(respuesta_span) - 1\n",
    "\n",
    "        # Agregar start_positions y end_positions\n",
    "        start_positions.append(respuesta_start_index)\n",
    "        end_positions.append(respuesta_end_index)\n",
    "\n",
    "# Convertir las listas en tensores\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "start_positions = torch.tensor(start_positions)\n",
    "end_positions = torch.tensor(end_positions)\n",
    "\n",
    "# Crear el DataLoader\n",
    "batch_size = 16\n",
    "train_data = TensorDataset(input_ids, attention_masks, start_positions, end_positions)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Optimizador\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Entrenamiento\n",
    "model.train()\n",
    "for epoch in range(4):  # Número de épocas\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_start_positions = batch[2]\n",
    "        b_end_positions = batch[3]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        start_positions=b_start_positions, \n",
    "                        end_positions=b_end_positions)\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualizar los parámetros y avanzar\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar el modelo ya entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "model.save_pretrained('model')\n",
    "\n",
    "# Cargar el modelo\n",
    "model = BertForQuestionAnswering.from_pretrained('model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacer predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: que subcampos abarca la inteligencia artificial ? [SEP] inteligen\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Formular una pregunta y su contexto\n",
    "pregunta = \"Que subcampos abarca la Inteligencia Artificial?\"\n",
    "contexto = \"inteligencia atificial\"\n",
    "\n",
    "# Preparar la entrada para el modelo\n",
    "inputs = tokenizer.encode_plus(pregunta, contexto, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "# Realizar la predicción\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Encontrar la posición de inicio y fin de la respuesta en el contexto\n",
    "answer_start = torch.argmax(outputs.start_logits)\n",
    "answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "# Convertir las posiciones de respuesta a texto\n",
    "respuesta = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end]))\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(\"Respuesta:\", respuesta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0]\n",
      "F1 Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calcular_f1(respuestas_modelo, respuestas_referencia):\n",
    "    # Asegúrate de que las respuestas estén en formato de lista y sean binarias (1 para correcto, 0 para incorrecto)\n",
    "    respuestas_modelo_binarias = [1 if respuesta in respuestas_referencia else 0 for respuesta in respuestas_modelo]\n",
    "    respuestas_referencia_binarias = [1 for _ in respuestas_referencia]\n",
    "    \n",
    "    return f1_score(respuestas_referencia_binarias, respuestas_modelo_binarias)\n",
    "\n",
    "def respuesta_es_correcta(respuesta_modelo, respuestas_referencia):\n",
    "    # Considera una respuesta correcta si coincide exactamente con alguna respuesta de referencia\n",
    "    return 1 if respuesta_modelo in respuestas_referencia else 0\n",
    "\n",
    "respuestas_modelo = [\n",
    "    \"respuesta 1 del modelo\",\n",
    "    \"respuesta 2 del modelo\",\n",
    "    \"respuesta 3 del modelo\",\n",
    "    # ... más respuestas\n",
    "]\n",
    "\n",
    "respuestas_referencia = [\n",
    "    \"respuesta 1 del modelo\",\n",
    "    \"respuesta 2 del modelo\",\n",
    "    \"respuesta correcta 3\",\n",
    "    # ... más respuestas de referencia\n",
    "]\n",
    "\n",
    "respuestas_modelo_binarias = [respuesta_es_correcta(respuesta, respuestas_referencia) for respuesta in respuestas_modelo]\n",
    "\n",
    "print(respuestas_modelo_binarias)\n",
    "\n",
    "f1 = calcular_f1(respuestas_modelo, respuestas_referencia)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
